<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="VuePress 2.0.0-beta.61">
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html, body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme');
			const systemDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
			if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
				document.documentElement.classList.toggle('dark', true);
			}
    </script>
    <title>目录 | 记录</title><meta name="description" content="个人博客">
    <link rel="preload" href="/notebook/assets/style-72c644a3.css" as="style"><link rel="stylesheet" href="/notebook/assets/style-72c644a3.css">
    <link rel="modulepreload" href="/notebook/assets/app-29aba0ee.js"><link rel="modulepreload" href="/notebook/assets/framework-e03faf0e.js"><link rel="modulepreload" href="/notebook/assets/Long-term Human Motion Prediction with Scene Context.html-a12c42cf.js"><link rel="modulepreload" href="/notebook/assets/Long-term Human Motion Prediction with Scene Context.html-ba27a589.js"><link rel="prefetch" href="/notebook/assets/index.html-2ace7bb6.js" as="script"><link rel="prefetch" href="/notebook/assets/python.html-f7046045.js" as="script"><link rel="prefetch" href="/notebook/assets/blog.html-cfba2365.js" as="script"><link rel="prefetch" href="/notebook/assets/git.html-20eabbb7.js" as="script"><link rel="prefetch" href="/notebook/assets/jetbrains.html-1049a7bd.js" as="script"><link rel="prefetch" href="/notebook/assets/obsidian配置.html-c2b33983.js" as="script"><link rel="prefetch" href="/notebook/assets/three.html-c47c34ac.js" as="script"><link rel="prefetch" href="/notebook/assets/virtualenv.html-f88fa800.js" as="script"><link rel="prefetch" href="/notebook/assets/2023.04.27目标检测概述.html-f3c2e35c.js" as="script"><link rel="prefetch" href="/notebook/assets/2023.05.04FPN.html-1bdbd047.js" as="script"><link rel="prefetch" href="/notebook/assets/2023.05.11动作预测.html-0622b710.js" as="script"><link rel="prefetch" href="/notebook/assets/数据结构.html-9e5934b3.js" as="script"><link rel="prefetch" href="/notebook/assets/Docker.html-af4b735b.js" as="script"><link rel="prefetch" href="/notebook/assets/Maven.html-9468c922.js" as="script"><link rel="prefetch" href="/notebook/assets/Nginx.html-d7cc1b84.js" as="script"><link rel="prefetch" href="/notebook/assets/bash.html-137e9ab2.js" as="script"><link rel="prefetch" href="/notebook/assets/shell.html-1e644b7f.js" as="script"><link rel="prefetch" href="/notebook/assets/vim.html-8ee138e1.js" as="script"><link rel="prefetch" href="/notebook/assets/快速入门.html-afc25d48.js" as="script"><link rel="prefetch" href="/notebook/assets/原理解析.html-0e682a48.js" as="script"><link rel="prefetch" href="/notebook/assets/小知识点.html-375042f2.js" as="script"><link rel="prefetch" href="/notebook/assets/注解开发.html-ef417a35.js" as="script"><link rel="prefetch" href="/notebook/assets/AOP.html-8d768895.js" as="script"><link rel="prefetch" href="/notebook/assets/一个spring程序.html-7684c74d.js" as="script"><link rel="prefetch" href="/notebook/assets/声明式事务.html-46f2b627.js" as="script"><link rel="prefetch" href="/notebook/assets/整合Mybatis.html-24016c8e.js" as="script"><link rel="prefetch" href="/notebook/assets/Eureka.html-8d1b43f3.js" as="script"><link rel="prefetch" href="/notebook/assets/Feign.html-4e8e78f5.js" as="script"><link rel="prefetch" href="/notebook/assets/Hystrix.html-4bd24289.js" as="script"><link rel="prefetch" href="/notebook/assets/Ribbon.html-53740665.js" as="script"><link rel="prefetch" href="/notebook/assets/Zuul.html-2498ee57.js" as="script"><link rel="prefetch" href="/notebook/assets/初识SpringCloud.html-3b80e5d5.js" as="script"><link rel="prefetch" href="/notebook/assets/Jdbc.html-a7e8f990.js" as="script"><link rel="prefetch" href="/notebook/assets/Shiro.html-e44467e8.js" as="script"><link rel="prefetch" href="/notebook/assets/Swagger.html-911f6af4.js" as="script"><link rel="prefetch" href="/notebook/assets/Web静态资源处理.html-1f68088e.js" as="script"><link rel="prefetch" href="/notebook/assets/thymeleaf.html-ac0435f9.js" as="script"><link rel="prefetch" href="/notebook/assets/分布式.html-e19b2a12.js" as="script"><link rel="prefetch" href="/notebook/assets/异步、定时、邮件任务.html-7faaa219.js" as="script"><link rel="prefetch" href="/notebook/assets/整合mybatis.html-7afca196.js" as="script"><link rel="prefetch" href="/notebook/assets/第一天.html-98b6564d.js" as="script"><link rel="prefetch" href="/notebook/assets/第二天.html-573e1be5.js" as="script"><link rel="prefetch" href="/notebook/assets/ES6补充.html-b4cd5561.js" as="script"><link rel="prefetch" href="/notebook/assets/Vue基础.html-515cde43.js" as="script"><link rel="prefetch" href="/notebook/assets/Vue项目部署.html-f336f953.js" as="script"><link rel="prefetch" href="/notebook/assets/threejs的使用.html-ee030932.js" as="script"><link rel="prefetch" href="/notebook/assets/单文件组件.html-ff377a86.js" as="script"><link rel="prefetch" href="/notebook/assets/脚手架.html-e0f489e2.js" as="script"><link rel="prefetch" href="/notebook/assets/页面title.html-172b29b7.js" as="script"><link rel="prefetch" href="/notebook/assets/Bitmaps.html-79c742a3.js" as="script"><link rel="prefetch" href="/notebook/assets/Hyperloglog.html-d03cd507.js" as="script"><link rel="prefetch" href="/notebook/assets/Redis的基本事务操作.html-53b5c206.js" as="script"><link rel="prefetch" href="/notebook/assets/redis-benchmark性能测试.html-04679c0f.js" as="script"><link rel="prefetch" href="/notebook/assets/基本知识点.html-45da19a4.js" as="script"><link rel="prefetch" href="/notebook/assets/安装.html-57f6a802.js" as="script"><link rel="prefetch" href="/notebook/assets/集成Redis.html-d8f8d67c.js" as="script"><link rel="prefetch" href="/notebook/assets/Center_pooling.html-85cfbb9a.js" as="script"><link rel="prefetch" href="/notebook/assets/Focal_Loss.html-193c6d7b.js" as="script"><link rel="prefetch" href="/notebook/assets/MAE.html-4931b3e8.js" as="script"><link rel="prefetch" href="/notebook/assets/NMS非极大值抑制.html-09b3f293.js" as="script"><link rel="prefetch" href="/notebook/assets/corner_pooling.html-1e393616.js" as="script"><link rel="prefetch" href="/notebook/assets/smooth_L1_loss.html-91533a5e.js" as="script"><link rel="prefetch" href="/notebook/assets/CMU motion capture.html-6d30a9bd.js" as="script"><link rel="prefetch" href="/notebook/assets/Human3.6M.html-f42ef51e.js" as="script"><link rel="prefetch" href="/notebook/assets/Adapters.html-dd2b3a22.js" as="script"><link rel="prefetch" href="/notebook/assets/CLIP.html-c1dfee82.js" as="script"><link rel="prefetch" href="/notebook/assets/Faster_RCNN.html-a55276c4.js" as="script"><link rel="prefetch" href="/notebook/assets/MoCo.html-bdef694d.js" as="script"><link rel="prefetch" href="/notebook/assets/RoI池化层.html-af6a69eb.js" as="script"><link rel="prefetch" href="/notebook/assets/TCN.html-fe2792d8.js" as="script"><link rel="prefetch" href="/notebook/assets/VAE.html-d465ab46.js" as="script"><link rel="prefetch" href="/notebook/assets/soft-argmax.html-e9d8f707.js" as="script"><link rel="prefetch" href="/notebook/assets/元学习.html-3bfa8b9a.js" as="script"><link rel="prefetch" href="/notebook/assets/反投影（back-projecting）.html-6f4e64f6.js" as="script"><link rel="prefetch" href="/notebook/assets/对比学习.html-5acba346.js" as="script"><link rel="prefetch" href="/notebook/assets/训练技巧.html-6010d278.js" as="script"><link rel="prefetch" href="/notebook/assets/选择性搜索.html-c12ad804.js" as="script"><link rel="prefetch" href="/notebook/assets/A convnet for the 2020s.html-2bc17d6d.js" as="script"><link rel="prefetch" href="/notebook/assets/Co-occurrence Feature Learning from Skeleton Data.html-0b119cdf.js" as="script"><link rel="prefetch" href="/notebook/assets/DETRs Beat YOLOs on Real-time Object Detection.html-059b7cc2.js" as="script"><link rel="prefetch" href="/notebook/assets/Disentangling Identity and Pose for Facial Expression Recognition.html-f06badbb.js" as="script"><link rel="prefetch" href="/notebook/assets/Dynamic Multiscale Graph Neural Networks for 3D Skeleton-Based Human Motion Prediction.html-6eabf3f7.js" as="script"><link rel="prefetch" href="/notebook/assets/EVOLVING REINFORCEMENT LEARNING ALGORITHMS.html-f3644afb.js" as="script"><link rel="prefetch" href="/notebook/assets/Lite DETR An Interleaved Multi-Scale Encoder for Efficient DETR.html-b8a67599.js" as="script"><link rel="prefetch" href="/notebook/assets/Real-time 2D Multi-Person Pose Estimation on CPU.html-5f3f363f.js" as="script"><link rel="prefetch" href="/notebook/assets/SSHFD Single Shot Human Fall Detection with Occlud.html-07421328.js" as="script"><link rel="prefetch" href="/notebook/assets/Scaling Up Your Kernels to 31x31 Revisiting Large.html-142b670f.js" as="script"><link rel="prefetch" href="/notebook/assets/SkeleMotion A New Representation of Skeleton Joint.html-da6c1581.js" as="script"><link rel="prefetch" href="/notebook/assets/Spatial Temporal Graph Convolutional Networks for.html-15a296fe.js" as="script"><link rel="prefetch" href="/notebook/assets/TSM Temporal Shift Module for Efficient Video Unde.html-ca0c4928.js" as="script"><link rel="prefetch" href="/notebook/assets/Towards Automated and Marker-less Parkinson Diseas.html-6428677a.js" as="script"><link rel="prefetch" href="/notebook/assets/Two-Stream Convolutional Networks for Action Recog.html-f82572e0.js" as="script"><link rel="prefetch" href="/notebook/assets/🌟LoGoNet_ Towards Accurate 3D Object Detection with Local-to-Global Cross-Modal Fusion.html-f6c8bb99.js" as="script"><link rel="prefetch" href="/notebook/assets/概述.html-a7e5df19.js" as="script"><link rel="prefetch" href="/notebook/assets/时间序列模型.html-9fc0dadf.js" as="script"><link rel="prefetch" href="/notebook/assets/线性代数.html-61a67473.js" as="script"><link rel="prefetch" href="/notebook/assets/存储器.html-2f2b73aa.js" as="script"><link rel="prefetch" href="/notebook/assets/系统总线.html-2d2d0cb1.js" as="script"><link rel="prefetch" href="/notebook/assets/计算机基本组成.html-1a73a14e.js" as="script"><link rel="prefetch" href="/notebook/assets/计算机的运算方法.html-b45a09c0.js" as="script"><link rel="prefetch" href="/notebook/assets/输入输出系统.html-f55069c6.js" as="script"><link rel="prefetch" href="/notebook/assets/高速缓存存储器.html-4125fa28.js" as="script"><link rel="prefetch" href="/notebook/assets/JDBC.html-1144b752.js" as="script"><link rel="prefetch" href="/notebook/assets/JSP.html-c6cac9bd.js" as="script"><link rel="prefetch" href="/notebook/assets/Servlet.html-9ba02088.js" as="script"><link rel="prefetch" href="/notebook/assets/Tomcat.html-06755505.js" as="script"><link rel="prefetch" href="/notebook/assets/监听器.html-de7634dd.js" as="script"><link rel="prefetch" href="/notebook/assets/缓存.html-7b0cf2e7.js" as="script"><link rel="prefetch" href="/notebook/assets/过滤器.html-e0793ac7.js" as="script"><link rel="prefetch" href="/notebook/assets/IO.html-2e0c5c17.js" as="script"><link rel="prefetch" href="/notebook/assets/反射.html-f7cdd524.js" as="script"><link rel="prefetch" href="/notebook/assets/容器.html-db6f490e.js" as="script"><link rel="prefetch" href="/notebook/assets/泛型.html-b1b6fc4a.js" as="script"><link rel="prefetch" href="/notebook/assets/注解.html-77360df3.js" as="script"><link rel="prefetch" href="/notebook/assets/概述.html-31096f5b.js" as="script"><link rel="prefetch" href="/notebook/assets/CRUD.html-94f6d3a4.js" as="script"><link rel="prefetch" href="/notebook/assets/ResultMap结果集映射.html-9375f298.js" as="script"><link rel="prefetch" href="/notebook/assets/lombok.html-00261dd2.js" as="script"><link rel="prefetch" href="/notebook/assets/一对多.html-86fe7b2a.js" as="script"><link rel="prefetch" href="/notebook/assets/分页.html-a20de2b4.js" as="script"><link rel="prefetch" href="/notebook/assets/动态SQL.html-d031d0b3.js" as="script"><link rel="prefetch" href="/notebook/assets/日志.html-5ed4396e.js" as="script"><link rel="prefetch" href="/notebook/assets/概念.html-34c34093.js" as="script"><link rel="prefetch" href="/notebook/assets/注解开发.html-ae7c8458.js" as="script"><link rel="prefetch" href="/notebook/assets/生命周期和作用域.html-86ba199e.js" as="script"><link rel="prefetch" href="/notebook/assets/第一个Mybatis程序.html-119f379c.js" as="script"><link rel="prefetch" href="/notebook/assets/缓存.html-62196616.js" as="script"><link rel="prefetch" href="/notebook/assets/配置解析.html-977e55e4.js" as="script"><link rel="prefetch" href="/notebook/assets/404.html-f9875e7b.js" as="script"><link rel="prefetch" href="/notebook/assets/index.html-ffe77b09.js" as="script"><link rel="prefetch" href="/notebook/assets/python.html-d21cf8d7.js" as="script"><link rel="prefetch" href="/notebook/assets/blog.html-08f51f62.js" as="script"><link rel="prefetch" href="/notebook/assets/git.html-efb6cc88.js" as="script"><link rel="prefetch" href="/notebook/assets/jetbrains.html-35e31cc3.js" as="script"><link rel="prefetch" href="/notebook/assets/obsidian配置.html-b8178469.js" as="script"><link rel="prefetch" href="/notebook/assets/three.html-21938d7b.js" as="script"><link rel="prefetch" href="/notebook/assets/virtualenv.html-5d77a6db.js" as="script"><link rel="prefetch" href="/notebook/assets/2023.04.27目标检测概述.html-16497519.js" as="script"><link rel="prefetch" href="/notebook/assets/2023.05.04FPN.html-f21d36d1.js" as="script"><link rel="prefetch" href="/notebook/assets/2023.05.11动作预测.html-806a0f9e.js" as="script"><link rel="prefetch" href="/notebook/assets/数据结构.html-92d7751b.js" as="script"><link rel="prefetch" href="/notebook/assets/Docker.html-3a6d2ed2.js" as="script"><link rel="prefetch" href="/notebook/assets/Maven.html-67a72842.js" as="script"><link rel="prefetch" href="/notebook/assets/Nginx.html-9d4a40d0.js" as="script"><link rel="prefetch" href="/notebook/assets/bash.html-b0cab74f.js" as="script"><link rel="prefetch" href="/notebook/assets/shell.html-8399ebb8.js" as="script"><link rel="prefetch" href="/notebook/assets/vim.html-075c198f.js" as="script"><link rel="prefetch" href="/notebook/assets/快速入门.html-b652b302.js" as="script"><link rel="prefetch" href="/notebook/assets/原理解析.html-453c0561.js" as="script"><link rel="prefetch" href="/notebook/assets/小知识点.html-576e16e5.js" as="script"><link rel="prefetch" href="/notebook/assets/注解开发.html-6060f006.js" as="script"><link rel="prefetch" href="/notebook/assets/AOP.html-6fa5fadb.js" as="script"><link rel="prefetch" href="/notebook/assets/一个spring程序.html-c21d7044.js" as="script"><link rel="prefetch" href="/notebook/assets/声明式事务.html-b2c0f311.js" as="script"><link rel="prefetch" href="/notebook/assets/整合Mybatis.html-a347acec.js" as="script"><link rel="prefetch" href="/notebook/assets/Eureka.html-244a3c0f.js" as="script"><link rel="prefetch" href="/notebook/assets/Feign.html-62490d03.js" as="script"><link rel="prefetch" href="/notebook/assets/Hystrix.html-902868c2.js" as="script"><link rel="prefetch" href="/notebook/assets/Ribbon.html-220ad043.js" as="script"><link rel="prefetch" href="/notebook/assets/Zuul.html-19734fdf.js" as="script"><link rel="prefetch" href="/notebook/assets/初识SpringCloud.html-ca90e03a.js" as="script"><link rel="prefetch" href="/notebook/assets/Jdbc.html-73f4bf71.js" as="script"><link rel="prefetch" href="/notebook/assets/Shiro.html-b5e42919.js" as="script"><link rel="prefetch" href="/notebook/assets/Swagger.html-163a1cd4.js" as="script"><link rel="prefetch" href="/notebook/assets/Web静态资源处理.html-4eacdfc4.js" as="script"><link rel="prefetch" href="/notebook/assets/thymeleaf.html-6fe41ccd.js" as="script"><link rel="prefetch" href="/notebook/assets/分布式.html-e4e536cf.js" as="script"><link rel="prefetch" href="/notebook/assets/异步、定时、邮件任务.html-e7fe2c2a.js" as="script"><link rel="prefetch" href="/notebook/assets/整合mybatis.html-dc407a1d.js" as="script"><link rel="prefetch" href="/notebook/assets/第一天.html-5b45ced2.js" as="script"><link rel="prefetch" href="/notebook/assets/第二天.html-5dda30f7.js" as="script"><link rel="prefetch" href="/notebook/assets/ES6补充.html-668234e1.js" as="script"><link rel="prefetch" href="/notebook/assets/Vue基础.html-91a7a31f.js" as="script"><link rel="prefetch" href="/notebook/assets/Vue项目部署.html-1c3eb1d8.js" as="script"><link rel="prefetch" href="/notebook/assets/threejs的使用.html-350e2816.js" as="script"><link rel="prefetch" href="/notebook/assets/单文件组件.html-5bc8b3be.js" as="script"><link rel="prefetch" href="/notebook/assets/脚手架.html-ca084d7d.js" as="script"><link rel="prefetch" href="/notebook/assets/页面title.html-979513ef.js" as="script"><link rel="prefetch" href="/notebook/assets/Bitmaps.html-b806a27f.js" as="script"><link rel="prefetch" href="/notebook/assets/Hyperloglog.html-ccbee38e.js" as="script"><link rel="prefetch" href="/notebook/assets/Redis的基本事务操作.html-9277f0e7.js" as="script"><link rel="prefetch" href="/notebook/assets/redis-benchmark性能测试.html-0c4eea37.js" as="script"><link rel="prefetch" href="/notebook/assets/基本知识点.html-2442010c.js" as="script"><link rel="prefetch" href="/notebook/assets/安装.html-44dabf62.js" as="script"><link rel="prefetch" href="/notebook/assets/集成Redis.html-e0fe8e54.js" as="script"><link rel="prefetch" href="/notebook/assets/Center_pooling.html-7434a9ae.js" as="script"><link rel="prefetch" href="/notebook/assets/Focal_Loss.html-c736ff86.js" as="script"><link rel="prefetch" href="/notebook/assets/MAE.html-4785c79d.js" as="script"><link rel="prefetch" href="/notebook/assets/NMS非极大值抑制.html-8694503d.js" as="script"><link rel="prefetch" href="/notebook/assets/corner_pooling.html-1e3cd173.js" as="script"><link rel="prefetch" href="/notebook/assets/smooth_L1_loss.html-f9ed61b5.js" as="script"><link rel="prefetch" href="/notebook/assets/CMU motion capture.html-e2773175.js" as="script"><link rel="prefetch" href="/notebook/assets/Human3.6M.html-1f34af7d.js" as="script"><link rel="prefetch" href="/notebook/assets/Adapters.html-46c37106.js" as="script"><link rel="prefetch" href="/notebook/assets/CLIP.html-03a7b405.js" as="script"><link rel="prefetch" href="/notebook/assets/Faster_RCNN.html-4c33a2fb.js" as="script"><link rel="prefetch" href="/notebook/assets/MoCo.html-99e46914.js" as="script"><link rel="prefetch" href="/notebook/assets/RoI池化层.html-ab6c63f5.js" as="script"><link rel="prefetch" href="/notebook/assets/TCN.html-81df50ac.js" as="script"><link rel="prefetch" href="/notebook/assets/VAE.html-ff059abf.js" as="script"><link rel="prefetch" href="/notebook/assets/soft-argmax.html-238b0d93.js" as="script"><link rel="prefetch" href="/notebook/assets/元学习.html-8e751b92.js" as="script"><link rel="prefetch" href="/notebook/assets/反投影（back-projecting）.html-3e3d03e7.js" as="script"><link rel="prefetch" href="/notebook/assets/对比学习.html-834d7bdc.js" as="script"><link rel="prefetch" href="/notebook/assets/训练技巧.html-856fd83a.js" as="script"><link rel="prefetch" href="/notebook/assets/选择性搜索.html-30e2c070.js" as="script"><link rel="prefetch" href="/notebook/assets/A convnet for the 2020s.html-901e6ef5.js" as="script"><link rel="prefetch" href="/notebook/assets/Co-occurrence Feature Learning from Skeleton Data.html-0e29fe70.js" as="script"><link rel="prefetch" href="/notebook/assets/DETRs Beat YOLOs on Real-time Object Detection.html-2021478c.js" as="script"><link rel="prefetch" href="/notebook/assets/Disentangling Identity and Pose for Facial Expression Recognition.html-23fc93ca.js" as="script"><link rel="prefetch" href="/notebook/assets/Dynamic Multiscale Graph Neural Networks for 3D Skeleton-Based Human Motion Prediction.html-65f68e7a.js" as="script"><link rel="prefetch" href="/notebook/assets/EVOLVING REINFORCEMENT LEARNING ALGORITHMS.html-847e3006.js" as="script"><link rel="prefetch" href="/notebook/assets/Lite DETR An Interleaved Multi-Scale Encoder for Efficient DETR.html-ed1f2a21.js" as="script"><link rel="prefetch" href="/notebook/assets/Real-time 2D Multi-Person Pose Estimation on CPU.html-00850641.js" as="script"><link rel="prefetch" href="/notebook/assets/SSHFD Single Shot Human Fall Detection with Occlud.html-fe4ba61a.js" as="script"><link rel="prefetch" href="/notebook/assets/Scaling Up Your Kernels to 31x31 Revisiting Large.html-9a8b9fcd.js" as="script"><link rel="prefetch" href="/notebook/assets/SkeleMotion A New Representation of Skeleton Joint.html-f48d52c4.js" as="script"><link rel="prefetch" href="/notebook/assets/Spatial Temporal Graph Convolutional Networks for.html-f7e0dab4.js" as="script"><link rel="prefetch" href="/notebook/assets/TSM Temporal Shift Module for Efficient Video Unde.html-e0f7bbc5.js" as="script"><link rel="prefetch" href="/notebook/assets/Towards Automated and Marker-less Parkinson Diseas.html-a172ea8a.js" as="script"><link rel="prefetch" href="/notebook/assets/Two-Stream Convolutional Networks for Action Recog.html-7320af12.js" as="script"><link rel="prefetch" href="/notebook/assets/🌟LoGoNet_ Towards Accurate 3D Object Detection with Local-to-Global Cross-Modal Fusion.html-164878e0.js" as="script"><link rel="prefetch" href="/notebook/assets/概述.html-82ca03b1.js" as="script"><link rel="prefetch" href="/notebook/assets/时间序列模型.html-5eb2c87a.js" as="script"><link rel="prefetch" href="/notebook/assets/线性代数.html-9d56d107.js" as="script"><link rel="prefetch" href="/notebook/assets/存储器.html-f1724edc.js" as="script"><link rel="prefetch" href="/notebook/assets/系统总线.html-65a88c47.js" as="script"><link rel="prefetch" href="/notebook/assets/计算机基本组成.html-a6e6ab30.js" as="script"><link rel="prefetch" href="/notebook/assets/计算机的运算方法.html-06419318.js" as="script"><link rel="prefetch" href="/notebook/assets/输入输出系统.html-70cdf8b9.js" as="script"><link rel="prefetch" href="/notebook/assets/高速缓存存储器.html-c82d9391.js" as="script"><link rel="prefetch" href="/notebook/assets/JDBC.html-db742cd3.js" as="script"><link rel="prefetch" href="/notebook/assets/JSP.html-5694e9e1.js" as="script"><link rel="prefetch" href="/notebook/assets/Servlet.html-3f528292.js" as="script"><link rel="prefetch" href="/notebook/assets/Tomcat.html-8f5df9f1.js" as="script"><link rel="prefetch" href="/notebook/assets/监听器.html-6dce5d6f.js" as="script"><link rel="prefetch" href="/notebook/assets/缓存.html-bde43d43.js" as="script"><link rel="prefetch" href="/notebook/assets/过滤器.html-f022c23a.js" as="script"><link rel="prefetch" href="/notebook/assets/IO.html-8ff1d015.js" as="script"><link rel="prefetch" href="/notebook/assets/反射.html-74e5bb6a.js" as="script"><link rel="prefetch" href="/notebook/assets/容器.html-eeb7b705.js" as="script"><link rel="prefetch" href="/notebook/assets/泛型.html-909eb974.js" as="script"><link rel="prefetch" href="/notebook/assets/注解.html-5e39bee7.js" as="script"><link rel="prefetch" href="/notebook/assets/概述.html-61749c53.js" as="script"><link rel="prefetch" href="/notebook/assets/CRUD.html-f0ca6cf6.js" as="script"><link rel="prefetch" href="/notebook/assets/ResultMap结果集映射.html-d3230fa0.js" as="script"><link rel="prefetch" href="/notebook/assets/lombok.html-27a4c600.js" as="script"><link rel="prefetch" href="/notebook/assets/一对多.html-3397922b.js" as="script"><link rel="prefetch" href="/notebook/assets/分页.html-9eec0880.js" as="script"><link rel="prefetch" href="/notebook/assets/动态SQL.html-8d138e1b.js" as="script"><link rel="prefetch" href="/notebook/assets/日志.html-3d803ee9.js" as="script"><link rel="prefetch" href="/notebook/assets/概念.html-78ac39d6.js" as="script"><link rel="prefetch" href="/notebook/assets/注解开发.html-88569228.js" as="script"><link rel="prefetch" href="/notebook/assets/生命周期和作用域.html-c5d10be9.js" as="script"><link rel="prefetch" href="/notebook/assets/第一个Mybatis程序.html-e9c69dc2.js" as="script"><link rel="prefetch" href="/notebook/assets/缓存.html-e08eaa72.js" as="script"><link rel="prefetch" href="/notebook/assets/配置解析.html-4ecd170f.js" as="script"><link rel="prefetch" href="/notebook/assets/404.html-4e6aa668.js" as="script"><link rel="prefetch" href="/notebook/assets/style-e9220a04.js" as="script"><link rel="prefetch" href="/notebook/assets/docsearch-1d421ddb.js" as="script"><link rel="prefetch" href="/notebook/assets/index-ade63bb5.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container"><!--[--><header class="navbar"><div class="toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a href="/notebook/" class=""><!----><span class="site-name">记录</span></a></span><div class="navbar-items-wrapper" style=""><!--[--><!--]--><nav class="navbar-items can-hide"><!--[--><div class="navbar-item"><a href="/notebook/知识/数据结构.html" class="" aria-label="测试"><!--[--><!--]--> 测试 <!--[--><!--]--></a></div><div class="navbar-item"><a class="external-link" href="https://github.com/dtenghao?tab=repositories" rel="noopener noreferrer" target="_blank" aria-label="GitHub"><!--[--><!--]--> GitHub <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></div><!--]--></nav><!--[--><!--]--><button class="toggle-color-mode-button" title="toggle color mode"><svg style="" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg style="display:none;" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!--[--><div id="docsearch-container" style="display:none;"></div><div><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"><svg width="15" height="15" class="DocSearch-Control-Key-Icon"><path d="M4.505 4.496h2M5.505 5.496v5M8.216 4.496l.055 5.993M10 7.5c.333.333.5.667.5 1v2M12.326 4.5v5.996M8.384 4.496c1.674 0 2.116 0 2.116 1.5s-.442 1.5-2.116 1.5M3.205 9.303c-.09.448-.277 1.21-1.241 1.203C1 10.5.5 9.513.5 8V7c0-1.57.5-2.5 1.464-2.494.964.006 1.134.598 1.24 1.342M12.553 10.5h1.953" stroke-width="1.2" stroke="currentColor" fill="none" stroke-linecap="square"></path></svg></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div></header><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar"><nav class="navbar-items"><!--[--><div class="navbar-item"><a href="/notebook/知识/数据结构.html" class="" aria-label="测试"><!--[--><!--]--> 测试 <!--[--><!--]--></a></div><div class="navbar-item"><a class="external-link" href="https://github.com/dtenghao?tab=repositories" rel="noopener noreferrer" target="_blank" aria-label="GitHub"><!--[--><!--]--> GitHub <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="sidebar-items"><!--[--><li><p tabindex="0" class="sidebar-item sidebar-heading">目录 <!----></p><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Long-term%20Human%20Motion%20Prediction%20with%20Scene%20Context.html#出发点" class="router-link-active router-link-exact-active sidebar-item" aria-label="出发点"><!--[--><!--]--> 出发点 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Long-term%20Human%20Motion%20Prediction%20with%20Scene%20Context.html#模型" class="router-link-active router-link-exact-active sidebar-item" aria-label="模型"><!--[--><!--]--> 模型 <!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Long-term%20Human%20Motion%20Prediction%20with%20Scene%20Context.html#goalnet" class="router-link-active router-link-exact-active sidebar-item" aria-label="GoalNet"><!--[--><!--]--> GoalNet <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Long-term%20Human%20Motion%20Prediction%20with%20Scene%20Context.html#pathnet" class="router-link-active router-link-exact-active sidebar-item" aria-label="PathNet"><!--[--><!--]--> PathNet <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Long-term%20Human%20Motion%20Prediction%20with%20Scene%20Context.html#posenet" class="router-link-active router-link-exact-active sidebar-item" aria-label="PoseNet"><!--[--><!--]--> PoseNet <!--[--><!--]--></a><!----></li><!--]--></ul></li><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Long-term%20Human%20Motion%20Prediction%20with%20Scene%20Context.html#数据集" class="router-link-active router-link-exact-active sidebar-item" aria-label="数据集"><!--[--><!--]--> 数据集 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Long-term%20Human%20Motion%20Prediction%20with%20Scene%20Context.html#实验" class="router-link-active router-link-exact-active sidebar-item" aria-label="实验"><!--[--><!--]--> 实验 <!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Long-term%20Human%20Motion%20Prediction%20with%20Scene%20Context.html#数据集-1" class="router-link-active router-link-exact-active sidebar-item" aria-label="数据集"><!--[--><!--]--> 数据集 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Long-term%20Human%20Motion%20Prediction%20with%20Scene%20Context.html#设置" class="router-link-active router-link-exact-active sidebar-item" aria-label="设置"><!--[--><!--]--> 设置 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Long-term%20Human%20Motion%20Prediction%20with%20Scene%20Context.html#方法比较" class="router-link-active router-link-exact-active sidebar-item" aria-label="方法比较"><!--[--><!--]--> 方法比较 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Long-term%20Human%20Motion%20Prediction%20with%20Scene%20Context.html#长期预测" class="router-link-active router-link-exact-active sidebar-item" aria-label="长期预测"><!--[--><!--]--> 长期预测 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Long-term%20Human%20Motion%20Prediction%20with%20Scene%20Context.html#讨论失败案例" class="router-link-active router-link-exact-active sidebar-item" aria-label="讨论失败案例"><!--[--><!--]--> 讨论失败案例 <!--[--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="page"><!--[--><!--]--><div class="theme-default-content"><!--[--><!--]--><div><blockquote><p>Cao, Zhe, et al. &quot;Long-term human motion prediction with scene context.&quot; <em>Computer Vision–ECCV 2020: 16 th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16</em>. Springer International Publishing, 2020.</p></blockquote><h1 id="出发点" tabindex="-1"><a class="header-anchor" href="#出发点" aria-hidden="true">#</a> 出发点</h1><p>现有的姿态预测问题都没有关注到<strong>场景信息</strong>，所以模型只能关注到骨架坐标等局部信息，难以做出包含更大的时空邻域的预测，因此在长期预测上表现都不好 本文模型框架在进行<strong>轨迹预测</strong>的基础上，进而进行<strong>姿态预测</strong>。 过去的轨迹预测使用物理力量、连续动态、隐马尔可夫模型或博弈论等方法也能取得不错的效果，但随着神经网络的兴起，场景和人物之间交互的多模态预测范式变得更加主流，但是他们都是使用鸟瞰图像或者已知的 3 D 信息，而本文使用多相机视角的图像 而过去的姿态预测则大都忽略了全局的环境因素，仅仅利用人体周围的局部图像环境来指导未来姿态的生成，这就容易导致预测的人体运动可能与场景不一致，例如，人可能会穿过墙壁行走。</p><hr><p>论文将动作预测的问题分为三个阶段：</p><ol><li>预测运动目标</li><li>规划每个目标之后的 3 D 路径</li><li>生成 3 D 姿态 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/7oOO2k.jpg" alt="7oOO2k"></li></ol><h1 id="模型" tabindex="-1"><a class="header-anchor" href="#模型" aria-hidden="true">#</a> 模型</h1><p>令 $\textbf{X}<em>{1:N}$ 为过去 $N$ 步的 2 D 姿态，$I$ 表示第 N 帧的场景，希望预测未来 $T$ 步 3 D 姿态，也就是 $\textbf{Y}</em>{N+1:N+T}$ 设人体骨架包含 $J$ 个关节，因此 $\textbf{X}\in\mathbb{R}^{J\times2}$，$\textbf{Y}\in\mathbb{R}^{J\times3}$，$\mathbf{X}^r$ 表示人体中心关节（躯干）的 2 D 坐标</p><hr><p>模型包括三个阶段：</p><ol><li><strong>GoalNet</strong>：在这个阶段，模型学习预测可能的人体运动目标，这些目标以图像空间中的 2 D 目的地来表示，基于 2 D 姿势历史和场景图像。</li><li><strong>PathNet</strong>：在这个阶段，模型学习计划通往每个目标的 3 D 路径，这是人体中心（躯干）的 3 D 位置序列，结合场景环境。这个路径考虑了环境约束，因此生成的运动路径更为合理。</li><li><strong>PoseNet</strong>：在这个阶段，模型预测每个时间步骤中，人体沿预测的 3 D 路径的 3 D 姿势。这样，得到的 3 D 人体运动具有全局移动，并考虑了周围场景，更为合理。 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/cCDfY7.jpg" alt="cCDfY7"></li></ol><h2 id="goalnet" tabindex="-1"><a class="header-anchor" href="#goalnet" aria-hidden="true">#</a> GoalNet</h2><p>为了理解人类的长期运动，我们必须根据目的地进行推理。而不是采用自回归模型来逐步生成姿势 $\hat{\textbf{X}}^r_{N+T}$ 表示可能的 2 D 目的地，将关节坐标 $\mathbf{X}^j$ 表示成热力图 $H^j$，热力图是以关节坐标为中心的高斯分布采样，以保持与图像上下文的空间相关性。 GoalNet 使用变分自编码器（<a href="/notebook/%E7%A7%91%E7%A0%94/%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/VAE.html" class="">VAE</a>）来实现，模型编码器输入热力图 $\textbf{H}<em>{1:N}^{1:J}$ 和场景图像 $I$ 获得潜在变量 $z$ 的分布 $$ \mathbf{z}\sim\mathcal{Q}(\mathbf{z}|\mathbf{H}</em>{1:N}^{1:J},\mathbf{I})\equiv\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\sigma}),\text{where}\boldsymbol{\mu},\boldsymbol{\sigma}=\mathcal{F}<em>{\text{enc}}(\mathbf{H}</em>{1:N}^{1:J},\mathbf{I}) $$ 然后通过采用潜在变量 $z$ ，并结合场景图像 $I$ 输入解码器中，输出目标的目的地热力图 $$ \hat{\textbf{H}}^r_{N+T}=\mathcal{F}<em>{\text{dec}}(\textbf{z},\textbf{I}) $$ 有了热力图，就可以通过 <a href="/notebook/%E7%A7%91%E7%A0%94/%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/soft-argmax.html" class="">soft-argmax</a>，来获得最终的 2 D 目的地 $\hat{\textbf{X}}^r</em>{N+T}$ ，因此对于整个 GoalNet，有： $$ \hat{\textbf{H}}^r_{N+T}=\mathcal{F}(\textbf{I},\textbf{H}^{1:j}<em>{1:N}) $$ 模型的损失函数包含<strong>目的地预测的 $l_1$ 误差</strong>，和潜在变量 $z$ 估计的 <strong>KL 散度</strong> $$ \begin{aligned} L</em>{\text{dest2D}}&amp; =|\textbf{X}<em>{N+T}^r-\hat{\textbf{X}}</em>{N+T}^r|<em>1 \ L</em>{\text{KL}}&amp; =\text{KL}\big[\mathcal{Q}\big(\text{z}|\text{H}<em>{1:N}^{1:J},\text{I}\big)||\mathcal{N}(0,1)\big] \end{aligned} $$ 两个损失相加时的权重相同。在推理阶段，从潜在变量 $z$ （$\mathcal N(\textbf{0},\textbf{1})$）中进行一组采样，结合场景图像 $I$ 就可以获得多个可能的 2 D 目的地 $\hat{\textbf{H}}^r</em>{N+T}$。 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/9ADgC5.jpg" alt="9ADgC5"><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/6hSZFv.jpg" alt="6hSZFv"></p><h2 id="pathnet" tabindex="-1"><a class="header-anchor" href="#pathnet" aria-hidden="true">#</a> PathNet</h2><p>前面预测的目标位置决定了移动的方向，而场景上下文决定了如何移动 在设计这个网络时，作者做了一个关键的决策，即不直接预测人体路径的 3 D 全局坐标值，而是将 3 D 路径表示为 2 D 路径热力图和人体中心随时间变化的深度值的组合。这种 3 D 路径表示方法有助于模型的训练。</p><ol><li><strong>预测目标位置</strong>：这一步的目标是预测人体的移动目标，即他们将移动到图像空间中的哪个位置。</li><li><strong>预测 3 D 路径</strong>：这一步的目标是预测人体的移动路径。这个路径是 3 D 的，包括每个时间步的人体中心位置。这个路径不仅取决于目标位置，也取决于场景的上下文。</li><li><strong>3 D 路径的表示</strong>：在这个方法中，3 D 路径不是直接表示为全局坐标值，而是表示为 2 D 路径热力图和人体中心的深度值的组合。2 D 路径热力图表示人体在图像平面上的移动，深度值表示人体在垂直于图像平面的方向上的移动。</li><li><strong>训练的便利性</strong>：这种 3 D 路径的表示方法更有利于模型的训练。因为热力图和深度值都可以用回归的方式来预测，这比直接预测 3 D 全局坐标值更容易。在作者的实验中，这种方法已经得到了验证。</li></ol><p>因此 PathNet $\phi$ 使用场景图像 $I$ 、2 D 姿态历史 $\textbf{H}<em>{1:N}^{1:J}$ 和 2 D 目的地 $\hat{\textbf{H}}^r</em>{N+T}$ 作为输入，预测 3 D 路径，表示成 $(\hat{\textbf{H}}^r_{N+1:N+T},\hat{\textbf{d}}^r_{1:N+T})$，$\hat{\textbf{d}}^r_{t}$ 表示人体中心在 $t$ 时刻的深度 $$ \hat{\textbf{H}}^r_{N+1:N+T},\hat{\textbf{d}}^r_{1:N+T}=\phi(\textbf{I},\textbf{X}^1_{1:N},\textbf{X}^r_{N+T}) $$ 最后通过利用相机内参 K 通过<a href="/notebook/%E7%A7%91%E7%A0%94/%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/%E5%8F%8D%E6%8A%95%E5%BD%B1%EF%BC%88back-projecting%EF%BC%89.html" class="">反投影（back-projecting）</a>来获得最终的 3 D 路径 $\hat{\textbf{Y}}^r_{1:N+T}$</p><p>作者使用 Hourglass 54 作为 PathNet 的 backbone，该网络有两个分支，其中第一个分支预测 2 D 路径热图，第二个分支预测人体躯干的深度 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/G45luE.jpg" alt="G45luE"> 损失函数为 <strong>2 D 路径预测误差</strong>和 <strong>3 D 路径误差</strong>，同时还需要惩罚连续帧之间的大位置变化 $$ L_{\text{path2D}}=|\mathbf{X}<em>{N+1:N+T}^r-\hat{\mathbf{X}}</em>{N+1:N+T}^r|<em>1 $$ $$ L</em>{\text{path3D}}=|\mathbf{Y}<em>{1:N+T}^r-\hat{\mathbf{Y}}</em>{1:N+T}^r|<em>1+|\hat{\mathbf{Y}}</em>{1:N+T-1}^r-\hat{\mathbf{Y}}_{2:N+T}|<em>1 $$ 在训练过程中，所有的损失函数都以相等的权重相加。在训练阶段，使用真实的目标位置（ground-truth destination）来训练 PathNet 模型。然而在测试阶段，他们可以使用 GoalNet 模型的预测结果作为目标位置 该模型共有随机、确定两种模式：第一种就是当前的模型，可以生成多个目的地；第二种是移除 GoalNet 和输入 $\textbf{X}^r</em>{N+T}$，而使用一个确定的目的地，这样 PathNet 就会生成一个确定的 3 D 轨迹预测 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/WlY5kO.jpg" alt="WlY5kO"></p><h2 id="posenet" tabindex="-1"><a class="header-anchor" href="#posenet" aria-hidden="true">#</a> PoseNet</h2><p>有了预测的 3 D 路径 $\mathbf{Y}<em>{1:N+T}^r$ 和 2 D 历史姿态 $X</em>{1: N}$，使用 transformer 作为 PoseNet $\Psi$ 模型来预测 3 D 姿态 作者将历史 2 D 姿态、人体躯干深度和相机内参 K 通过 <a href="/notebook/%E7%A7%91%E7%A0%94/%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/%E5%8F%8D%E6%8A%95%E5%BD%B1%EF%BC%88back-projecting%EF%BC%89.html" class="">反投影（back-projecting）</a>来获得带有噪声的 3 D 姿态 $\bar{\textbf{Y}}<em>{1:N}$，然后将 $\bar{\textbf{Y}}</em>{N}$ 重复复制到每个预测的未来 3 D 路径位置，来初步估计未来 3 D 姿态 $\bar{\textbf{Y}}<em>{N+1:N+T}$，最后将整个 3 D 姿态 $\bar{\textbf{Y}}</em>{1:N+T}$ 输入到 PoseNet $$ \hat{\textbf{Y}}<em>{N+1:N+T}=\Psi(\bar{\textbf{Y}}</em>{1:N+T}) $$ PoseNet 的训练目标是最小化 3 D 姿态预测与 ground-truth 之间的距离，其定义为： $$ L_{\text{pose3D}}=|\mathbf{Y}<em>{N+1:N+T}-\hat{\mathbf{Y}}</em>{N+1:N+T}|<em>1 $$ 在训练阶段，使用真实 (ground-truth)的 3 D 路径 $\textbf{Y}</em>{1:N+T}^r$，来估计 3 D 姿态输入。在推理阶段，则使用 PathNet 预测的 $\mathbf{Y}_{1:N+T}^r$ <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/WWuYX8.jpg" alt="WWuYX8"></p><h1 id="数据集" tabindex="-1"><a class="header-anchor" href="#数据集" aria-hidden="true">#</a> 数据集</h1><p>现有的真实人-场景交互数据集的<strong>3 D 人体姿态标注相对嘈杂</strong>，并且由于深度传感器的限制，长范围的人体运动数据有限。另一方面，现有的合成人体数据集主要关注人体姿态估计或部分分割任务，并且在开阔的户外场景中采样数据，这些场景的<strong>可交互物体有限</strong>。 为了解决上述问题，作者们进行了大量工作，通过开发一个与游戏引擎交互的接口，以全自动的方式控制角色、相机和行动任务，收集了一个合成数据集。对于每个角色，他们在 3 D 场景中随机设置目标地点，确定特定的任务、行走风格和移动速度。他们通过改变不同的天气条件和白天时间来控制光照条件。他们还多样化了相机的位置和视角，使其围绕演员在一个球体上变化，并始终指向演员。他们使用游戏内的光线追踪 API 和同步的人体分割图来跟踪演员。收集的动作包括上楼梯、躺下、坐下、开门等在室内场景中的基本活动。例如，角色有 22 种行走风格，包括 10 种女性和 12 种男性的行走风格。 <a href="https://github.com/ZheC/GTA-IM-Dataset" target="_blank" rel="noopener noreferrer">数据集<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>包含了<strong>一百万</strong>帧 RGBD 图像，分辨率为 1920×1080，每帧都有真实的 2 D/3 D 人体姿态（<strong>98</strong>个关节）、深度图、人体分割和相机姿态的标注。这个数据集包含了<strong>50</strong>个人物角色在<strong>10</strong>个不同的大型室内场景中的行为。每个场景都有几层楼，包括客厅、卧室、厨房、阳台等，可以进行多种交互活动。 <img src="/notebook/assets/vis_skeleton_pcd-e89f9950.gif" alt=""><img src="/notebook/assets/vis_2d_pose_depth-b9ffba09.gif" alt=""></p><h1 id="实验" tabindex="-1"><a class="header-anchor" href="#实验" aria-hidden="true">#</a> 实验</h1><h2 id="数据集-1" tabindex="-1"><a class="header-anchor" href="#数据集-1" aria-hidden="true">#</a> 数据集</h2><p><strong>GTA-IM</strong>：使用 8 个场景来训练，2 个场景来测试。在 98 个关节中选择 21 个，将三维路径和三维姿态转换为相机坐标帧，用于训练和评估 <strong>PROX</strong>：由 12 个不同的 3 D 场景和 20 个对象的 RGB 序列组成，这些对象在场景中移动并与场景互动。将数据集分成 52 个训练序列和 8 个用于测试的序列，并使用 18 个关节。</p><h2 id="设置" tabindex="-1"><a class="header-anchor" href="#设置" aria-hidden="true">#</a> 设置</h2><ul><li>输入图像大小和热力图大小为 256×448</li><li>输出未来热力图预测的分辨率为 64×112</li><li>所有深度维度值在训练过程中都被限制在 10 以内，并通过 4 进行归一化</li><li>分别训练所有 3 个模块（比联合训练效果更好）</li></ul><p><strong>GoalNet:</strong></p><ol><li>学习率设置为 10^(-4)，不使用权重衰减。</li><li>对于两个数据集，使用 128 的批量大小进行 2 个 epoch 的训练。</li></ol><p><strong>PathNet:</strong></p><ol><li>使用真实目标位置输入训练 PathNet，而在推理过程中，使用 GoalNet 的预测结果。</li><li>学习率设置为 2.5 × 10^(-4)，权重衰减为 10^(-4)。</li><li>对 GTA-IM 数据集进行 10 个 epoch 的训练，对 PROX 数据集进行 6 个 epoch 的训练，分别在第 7 个和第 4 个 epoch 时，学习率衰减为原来的10倍。</li><li>使用批量大小为 32。</li></ol><p><strong>PoseNet:</strong></p><ol><li>使用真实的 3 D 路径训练 PoseNet，而在推理过程中，使用 PoseNet 的预测结果。</li><li>使用学习率为 1 × 10^(-3)进行 80 个 epoch 的训练。</li><li>注意力丢失率为 0.2。</li><li>批量大小为 1024。</li></ol><h2 id="方法比较" tabindex="-1"><a class="header-anchor" href="#方法比较" aria-hidden="true">#</a> 方法比较</h2><p><strong>评价指标</strong>：将 Mean Per Joint Position Error (MPJPE)，计算预测的 3 D 关节位置和对应的真实关节位置之间的欧氏距离的平均值，作为评价指标来评估预测的 3 D 路径和姿态 在<strong>GTA-IM</strong>数据集上，模型的确定模式取得了最好的表现，且拉了其他方法一大截。而对于随机模式，选择一个损失最小的目的地，作者通过尝试生成不同数量的目的地，发现当生成 4 个可能的目的地时，模型表现可以接近确定模式，当生成 10 个可能的目的地时，模型表现已经开始优于确定模式</p><ol><li>他们首先进行了一个实验，直接回归 3 D 坐标（&quot;Ours w/ xyz.&quot;），结果发现总误差比确定性模型（&quot;deterministic model&quot;）高出 18 毫米。这验证了将 3 D 路径表示为人体中心的深度和 2 D 热图的方法更好，因为这种表示与图像外观有很强的相关性。</li><li>他们也进行了一些输入变量的消融实验。如果不输入图像，平均误差会高出 23 毫米。如果只输入遮罩图像，即将人体裁剪外的像素替换为 ImageNet 的平均像素值，误差会高出 16 毫米。这验证了使用全图像来编码场景上下文比仅观察裁剪的人体图像更有帮助，尤其是对于长期预测。</li><li>使用颜色和深度图像作为输入（&quot;Ours w/ RGBD input&quot;），平均误差为 172 毫米，与仅使用 RGB 输入的模型相近。这表明他们的模型隐含地从 2 D 输入中学习推理深度信息。</li><li>如果使用真实的 2 D 目标而不是预测的目标，总误差会降低到 137 毫米。这说明未来目标的不确定性是这个问题的主要难点。 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/iKbdwH.jpg" alt="iKbdwH"></li></ol><hr><p><strong>PROX</strong>数据集是一种真实世界的数据集，比 GTA-IM 更具挑战性，因为未来的运动不确定性更大。</p><ol><li>表 2 的评估结果显示，他们的方法在所有时间步的平均 MPJPE 方面超过了之前的最优结果，从 282 毫米降到了 270 毫米。</li><li>在 GTA-IM 数据集的比较中，他们得出的结论在 PROX 数据集中也得到了验证。当在推理过程中使用抽样时，进行三次抽样的推理性能可以超过确定性模型（264 毫米对比 270 毫米），当进行十次抽样时，误差进一步降低到 249 毫米。在 PROX 数据集上的这些改进比在 GTA-IM 基准测试中观察到的更为显著，原因是真实数据集中未来运动的不确定性更大，因此随机预测有更大的优势。</li><li>他们发现，在 GTA-IM 数据集上进行预训练可以获得更好的性能（270 毫米对比 280 毫米）。他们的方法利用了图像上下文，并且在 PROX 数据集上倾向于过拟合，因为在摄像机姿势和背景外观（在每个视频序列中都是恒定的）方面，该数据集的多样性较差。在他们具有多样外观和清晰注释的合成数据集上进行预训练，可以帮助防止过拟合，提高最终性能。 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/9HbuNz.jpg" alt="9HbuNz"></li></ol><h2 id="长期预测" tabindex="-1"><a class="header-anchor" href="#长期预测" aria-hidden="true">#</a> 长期预测</h2><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/WdDCUT.jpg" alt="WdDCUT"> 随机模型可以通过使用 5 个样本击败他们的确定性模型。随着样本数量的增加，测试误差相应地减小。在预测的后期阶段，确定性结果和随机结果之间的误差差距变得更大，即在 3 秒的时间步长上，差距超过 100 毫米。这表明随机模型在长期未来运动预测中的优势。</p><h2 id="讨论失败案例" tabindex="-1"><a class="header-anchor" href="#讨论失败案例" aria-hidden="true">#</a> 讨论失败案例</h2><p>本模型通过从大量训练数据中以数据驱动的方式隐含地学习场景约束，并且能产生相较于之前没有考虑场景上下文的方法更一致的 3 D 人体路径，且没有严重的碰撞，这表明模型能够理解和应用场景约束，以此产生在现实场景中更可能发生的人体运动 然而，由于他们的模型并未假设已经接触到预先重建的 3 D 网格，所以产生的 3 D 姿态可能不会严格满足 3 D 场景几何的所有物理约束。 因此作者认为这个问题可以通过将多视角/时间序列图像作为输入来解决，以更好地恢复 3 D 场景。结果的 3 D 场景可以进一步用来强制执行明确的场景几何约束，以优化 3 D 姿态。这是他们将来的工作方向。</p></div><!--[--><!--]--></div><footer class="page-meta"><div class="meta-item edit-link"><a class="external-link meta-item-label" href="https://github.com/dtenghao?tab=repositories/edit/main/科研/论文笔记/Long-term Human Motion Prediction with Scene Context.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page"><!--[--><!--]--> Edit this page <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></div><!----><!----></footer><!----><!--[--><!--]--></main><!--]--></div><!----><!--]--></div>
    <script type="module" src="/notebook/assets/app-29aba0ee.js" defer></script>
  </body>
</html>
