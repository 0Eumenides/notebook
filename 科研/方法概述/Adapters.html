<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="VuePress 2.0.0-beta.61">
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html, body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme');
			const systemDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
			if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
				document.documentElement.classList.toggle('dark', true);
			}
    </script>
    <title>目录 | 记录</title><meta name="description" content="个人博客">
    <link rel="preload" href="/notebook/assets/style-72c644a3.css" as="style"><link rel="stylesheet" href="/notebook/assets/style-72c644a3.css">
    <link rel="modulepreload" href="/notebook/assets/app-29aba0ee.js"><link rel="modulepreload" href="/notebook/assets/framework-e03faf0e.js"><link rel="modulepreload" href="/notebook/assets/Adapters.html-46c37106.js"><link rel="modulepreload" href="/notebook/assets/Adapters.html-dd2b3a22.js"><link rel="prefetch" href="/notebook/assets/index.html-2ace7bb6.js" as="script"><link rel="prefetch" href="/notebook/assets/python.html-f7046045.js" as="script"><link rel="prefetch" href="/notebook/assets/blog.html-cfba2365.js" as="script"><link rel="prefetch" href="/notebook/assets/git.html-20eabbb7.js" as="script"><link rel="prefetch" href="/notebook/assets/jetbrains.html-1049a7bd.js" as="script"><link rel="prefetch" href="/notebook/assets/obsidian配置.html-c2b33983.js" as="script"><link rel="prefetch" href="/notebook/assets/three.html-c47c34ac.js" as="script"><link rel="prefetch" href="/notebook/assets/virtualenv.html-f88fa800.js" as="script"><link rel="prefetch" href="/notebook/assets/2023.04.27目标检测概述.html-f3c2e35c.js" as="script"><link rel="prefetch" href="/notebook/assets/2023.05.04FPN.html-1bdbd047.js" as="script"><link rel="prefetch" href="/notebook/assets/2023.05.11动作预测.html-0622b710.js" as="script"><link rel="prefetch" href="/notebook/assets/数据结构.html-9e5934b3.js" as="script"><link rel="prefetch" href="/notebook/assets/Docker.html-af4b735b.js" as="script"><link rel="prefetch" href="/notebook/assets/Maven.html-9468c922.js" as="script"><link rel="prefetch" href="/notebook/assets/Nginx.html-d7cc1b84.js" as="script"><link rel="prefetch" href="/notebook/assets/bash.html-137e9ab2.js" as="script"><link rel="prefetch" href="/notebook/assets/shell.html-1e644b7f.js" as="script"><link rel="prefetch" href="/notebook/assets/vim.html-8ee138e1.js" as="script"><link rel="prefetch" href="/notebook/assets/快速入门.html-afc25d48.js" as="script"><link rel="prefetch" href="/notebook/assets/原理解析.html-0e682a48.js" as="script"><link rel="prefetch" href="/notebook/assets/小知识点.html-375042f2.js" as="script"><link rel="prefetch" href="/notebook/assets/注解开发.html-ef417a35.js" as="script"><link rel="prefetch" href="/notebook/assets/AOP.html-8d768895.js" as="script"><link rel="prefetch" href="/notebook/assets/一个spring程序.html-7684c74d.js" as="script"><link rel="prefetch" href="/notebook/assets/声明式事务.html-46f2b627.js" as="script"><link rel="prefetch" href="/notebook/assets/整合Mybatis.html-24016c8e.js" as="script"><link rel="prefetch" href="/notebook/assets/Eureka.html-8d1b43f3.js" as="script"><link rel="prefetch" href="/notebook/assets/Feign.html-4e8e78f5.js" as="script"><link rel="prefetch" href="/notebook/assets/Hystrix.html-4bd24289.js" as="script"><link rel="prefetch" href="/notebook/assets/Ribbon.html-53740665.js" as="script"><link rel="prefetch" href="/notebook/assets/Zuul.html-2498ee57.js" as="script"><link rel="prefetch" href="/notebook/assets/初识SpringCloud.html-3b80e5d5.js" as="script"><link rel="prefetch" href="/notebook/assets/Jdbc.html-a7e8f990.js" as="script"><link rel="prefetch" href="/notebook/assets/Shiro.html-e44467e8.js" as="script"><link rel="prefetch" href="/notebook/assets/Swagger.html-911f6af4.js" as="script"><link rel="prefetch" href="/notebook/assets/Web静态资源处理.html-1f68088e.js" as="script"><link rel="prefetch" href="/notebook/assets/thymeleaf.html-ac0435f9.js" as="script"><link rel="prefetch" href="/notebook/assets/分布式.html-e19b2a12.js" as="script"><link rel="prefetch" href="/notebook/assets/异步、定时、邮件任务.html-7faaa219.js" as="script"><link rel="prefetch" href="/notebook/assets/整合mybatis.html-7afca196.js" as="script"><link rel="prefetch" href="/notebook/assets/第一天.html-98b6564d.js" as="script"><link rel="prefetch" href="/notebook/assets/第二天.html-573e1be5.js" as="script"><link rel="prefetch" href="/notebook/assets/ES6补充.html-b4cd5561.js" as="script"><link rel="prefetch" href="/notebook/assets/Vue基础.html-515cde43.js" as="script"><link rel="prefetch" href="/notebook/assets/Vue项目部署.html-f336f953.js" as="script"><link rel="prefetch" href="/notebook/assets/threejs的使用.html-ee030932.js" as="script"><link rel="prefetch" href="/notebook/assets/单文件组件.html-ff377a86.js" as="script"><link rel="prefetch" href="/notebook/assets/脚手架.html-e0f489e2.js" as="script"><link rel="prefetch" href="/notebook/assets/页面title.html-172b29b7.js" as="script"><link rel="prefetch" href="/notebook/assets/Bitmaps.html-79c742a3.js" as="script"><link rel="prefetch" href="/notebook/assets/Hyperloglog.html-d03cd507.js" as="script"><link rel="prefetch" href="/notebook/assets/Redis的基本事务操作.html-53b5c206.js" as="script"><link rel="prefetch" href="/notebook/assets/redis-benchmark性能测试.html-04679c0f.js" as="script"><link rel="prefetch" href="/notebook/assets/基本知识点.html-45da19a4.js" as="script"><link rel="prefetch" href="/notebook/assets/安装.html-57f6a802.js" as="script"><link rel="prefetch" href="/notebook/assets/集成Redis.html-d8f8d67c.js" as="script"><link rel="prefetch" href="/notebook/assets/Center_pooling.html-85cfbb9a.js" as="script"><link rel="prefetch" href="/notebook/assets/Focal_Loss.html-193c6d7b.js" as="script"><link rel="prefetch" href="/notebook/assets/MAE.html-4931b3e8.js" as="script"><link rel="prefetch" href="/notebook/assets/NMS非极大值抑制.html-09b3f293.js" as="script"><link rel="prefetch" href="/notebook/assets/corner_pooling.html-1e393616.js" as="script"><link rel="prefetch" href="/notebook/assets/smooth_L1_loss.html-91533a5e.js" as="script"><link rel="prefetch" href="/notebook/assets/CMU motion capture.html-6d30a9bd.js" as="script"><link rel="prefetch" href="/notebook/assets/Human3.6M.html-f42ef51e.js" as="script"><link rel="prefetch" href="/notebook/assets/CLIP.html-c1dfee82.js" as="script"><link rel="prefetch" href="/notebook/assets/Faster_RCNN.html-a55276c4.js" as="script"><link rel="prefetch" href="/notebook/assets/MoCo.html-bdef694d.js" as="script"><link rel="prefetch" href="/notebook/assets/RoI池化层.html-af6a69eb.js" as="script"><link rel="prefetch" href="/notebook/assets/TCN.html-fe2792d8.js" as="script"><link rel="prefetch" href="/notebook/assets/VAE.html-d465ab46.js" as="script"><link rel="prefetch" href="/notebook/assets/soft-argmax.html-e9d8f707.js" as="script"><link rel="prefetch" href="/notebook/assets/元学习.html-3bfa8b9a.js" as="script"><link rel="prefetch" href="/notebook/assets/反投影（back-projecting）.html-6f4e64f6.js" as="script"><link rel="prefetch" href="/notebook/assets/对比学习.html-5acba346.js" as="script"><link rel="prefetch" href="/notebook/assets/训练技巧.html-6010d278.js" as="script"><link rel="prefetch" href="/notebook/assets/选择性搜索.html-c12ad804.js" as="script"><link rel="prefetch" href="/notebook/assets/A convnet for the 2020s.html-2bc17d6d.js" as="script"><link rel="prefetch" href="/notebook/assets/Co-occurrence Feature Learning from Skeleton Data.html-0b119cdf.js" as="script"><link rel="prefetch" href="/notebook/assets/DETRs Beat YOLOs on Real-time Object Detection.html-059b7cc2.js" as="script"><link rel="prefetch" href="/notebook/assets/Disentangling Identity and Pose for Facial Expression Recognition.html-f06badbb.js" as="script"><link rel="prefetch" href="/notebook/assets/Dynamic Multiscale Graph Neural Networks for 3D Skeleton-Based Human Motion Prediction.html-6eabf3f7.js" as="script"><link rel="prefetch" href="/notebook/assets/EVOLVING REINFORCEMENT LEARNING ALGORITHMS.html-f3644afb.js" as="script"><link rel="prefetch" href="/notebook/assets/Lite DETR An Interleaved Multi-Scale Encoder for Efficient DETR.html-b8a67599.js" as="script"><link rel="prefetch" href="/notebook/assets/Long-term Human Motion Prediction with Scene Context.html-ba27a589.js" as="script"><link rel="prefetch" href="/notebook/assets/Real-time 2D Multi-Person Pose Estimation on CPU.html-5f3f363f.js" as="script"><link rel="prefetch" href="/notebook/assets/SSHFD Single Shot Human Fall Detection with Occlud.html-07421328.js" as="script"><link rel="prefetch" href="/notebook/assets/Scaling Up Your Kernels to 31x31 Revisiting Large.html-142b670f.js" as="script"><link rel="prefetch" href="/notebook/assets/SkeleMotion A New Representation of Skeleton Joint.html-da6c1581.js" as="script"><link rel="prefetch" href="/notebook/assets/Spatial Temporal Graph Convolutional Networks for.html-15a296fe.js" as="script"><link rel="prefetch" href="/notebook/assets/TSM Temporal Shift Module for Efficient Video Unde.html-ca0c4928.js" as="script"><link rel="prefetch" href="/notebook/assets/Towards Automated and Marker-less Parkinson Diseas.html-6428677a.js" as="script"><link rel="prefetch" href="/notebook/assets/Two-Stream Convolutional Networks for Action Recog.html-f82572e0.js" as="script"><link rel="prefetch" href="/notebook/assets/🌟LoGoNet_ Towards Accurate 3D Object Detection with Local-to-Global Cross-Modal Fusion.html-f6c8bb99.js" as="script"><link rel="prefetch" href="/notebook/assets/概述.html-a7e5df19.js" as="script"><link rel="prefetch" href="/notebook/assets/时间序列模型.html-9fc0dadf.js" as="script"><link rel="prefetch" href="/notebook/assets/线性代数.html-61a67473.js" as="script"><link rel="prefetch" href="/notebook/assets/存储器.html-2f2b73aa.js" as="script"><link rel="prefetch" href="/notebook/assets/系统总线.html-2d2d0cb1.js" as="script"><link rel="prefetch" href="/notebook/assets/计算机基本组成.html-1a73a14e.js" as="script"><link rel="prefetch" href="/notebook/assets/计算机的运算方法.html-b45a09c0.js" as="script"><link rel="prefetch" href="/notebook/assets/输入输出系统.html-f55069c6.js" as="script"><link rel="prefetch" href="/notebook/assets/高速缓存存储器.html-4125fa28.js" as="script"><link rel="prefetch" href="/notebook/assets/JDBC.html-1144b752.js" as="script"><link rel="prefetch" href="/notebook/assets/JSP.html-c6cac9bd.js" as="script"><link rel="prefetch" href="/notebook/assets/Servlet.html-9ba02088.js" as="script"><link rel="prefetch" href="/notebook/assets/Tomcat.html-06755505.js" as="script"><link rel="prefetch" href="/notebook/assets/监听器.html-de7634dd.js" as="script"><link rel="prefetch" href="/notebook/assets/缓存.html-7b0cf2e7.js" as="script"><link rel="prefetch" href="/notebook/assets/过滤器.html-e0793ac7.js" as="script"><link rel="prefetch" href="/notebook/assets/IO.html-2e0c5c17.js" as="script"><link rel="prefetch" href="/notebook/assets/反射.html-f7cdd524.js" as="script"><link rel="prefetch" href="/notebook/assets/容器.html-db6f490e.js" as="script"><link rel="prefetch" href="/notebook/assets/泛型.html-b1b6fc4a.js" as="script"><link rel="prefetch" href="/notebook/assets/注解.html-77360df3.js" as="script"><link rel="prefetch" href="/notebook/assets/概述.html-31096f5b.js" as="script"><link rel="prefetch" href="/notebook/assets/CRUD.html-94f6d3a4.js" as="script"><link rel="prefetch" href="/notebook/assets/ResultMap结果集映射.html-9375f298.js" as="script"><link rel="prefetch" href="/notebook/assets/lombok.html-00261dd2.js" as="script"><link rel="prefetch" href="/notebook/assets/一对多.html-86fe7b2a.js" as="script"><link rel="prefetch" href="/notebook/assets/分页.html-a20de2b4.js" as="script"><link rel="prefetch" href="/notebook/assets/动态SQL.html-d031d0b3.js" as="script"><link rel="prefetch" href="/notebook/assets/日志.html-5ed4396e.js" as="script"><link rel="prefetch" href="/notebook/assets/概念.html-34c34093.js" as="script"><link rel="prefetch" href="/notebook/assets/注解开发.html-ae7c8458.js" as="script"><link rel="prefetch" href="/notebook/assets/生命周期和作用域.html-86ba199e.js" as="script"><link rel="prefetch" href="/notebook/assets/第一个Mybatis程序.html-119f379c.js" as="script"><link rel="prefetch" href="/notebook/assets/缓存.html-62196616.js" as="script"><link rel="prefetch" href="/notebook/assets/配置解析.html-977e55e4.js" as="script"><link rel="prefetch" href="/notebook/assets/404.html-f9875e7b.js" as="script"><link rel="prefetch" href="/notebook/assets/index.html-ffe77b09.js" as="script"><link rel="prefetch" href="/notebook/assets/python.html-d21cf8d7.js" as="script"><link rel="prefetch" href="/notebook/assets/blog.html-08f51f62.js" as="script"><link rel="prefetch" href="/notebook/assets/git.html-efb6cc88.js" as="script"><link rel="prefetch" href="/notebook/assets/jetbrains.html-35e31cc3.js" as="script"><link rel="prefetch" href="/notebook/assets/obsidian配置.html-b8178469.js" as="script"><link rel="prefetch" href="/notebook/assets/three.html-21938d7b.js" as="script"><link rel="prefetch" href="/notebook/assets/virtualenv.html-5d77a6db.js" as="script"><link rel="prefetch" href="/notebook/assets/2023.04.27目标检测概述.html-16497519.js" as="script"><link rel="prefetch" href="/notebook/assets/2023.05.04FPN.html-f21d36d1.js" as="script"><link rel="prefetch" href="/notebook/assets/2023.05.11动作预测.html-806a0f9e.js" as="script"><link rel="prefetch" href="/notebook/assets/数据结构.html-92d7751b.js" as="script"><link rel="prefetch" href="/notebook/assets/Docker.html-3a6d2ed2.js" as="script"><link rel="prefetch" href="/notebook/assets/Maven.html-67a72842.js" as="script"><link rel="prefetch" href="/notebook/assets/Nginx.html-9d4a40d0.js" as="script"><link rel="prefetch" href="/notebook/assets/bash.html-b0cab74f.js" as="script"><link rel="prefetch" href="/notebook/assets/shell.html-8399ebb8.js" as="script"><link rel="prefetch" href="/notebook/assets/vim.html-075c198f.js" as="script"><link rel="prefetch" href="/notebook/assets/快速入门.html-b652b302.js" as="script"><link rel="prefetch" href="/notebook/assets/原理解析.html-453c0561.js" as="script"><link rel="prefetch" href="/notebook/assets/小知识点.html-576e16e5.js" as="script"><link rel="prefetch" href="/notebook/assets/注解开发.html-6060f006.js" as="script"><link rel="prefetch" href="/notebook/assets/AOP.html-6fa5fadb.js" as="script"><link rel="prefetch" href="/notebook/assets/一个spring程序.html-c21d7044.js" as="script"><link rel="prefetch" href="/notebook/assets/声明式事务.html-b2c0f311.js" as="script"><link rel="prefetch" href="/notebook/assets/整合Mybatis.html-a347acec.js" as="script"><link rel="prefetch" href="/notebook/assets/Eureka.html-244a3c0f.js" as="script"><link rel="prefetch" href="/notebook/assets/Feign.html-62490d03.js" as="script"><link rel="prefetch" href="/notebook/assets/Hystrix.html-902868c2.js" as="script"><link rel="prefetch" href="/notebook/assets/Ribbon.html-220ad043.js" as="script"><link rel="prefetch" href="/notebook/assets/Zuul.html-19734fdf.js" as="script"><link rel="prefetch" href="/notebook/assets/初识SpringCloud.html-ca90e03a.js" as="script"><link rel="prefetch" href="/notebook/assets/Jdbc.html-73f4bf71.js" as="script"><link rel="prefetch" href="/notebook/assets/Shiro.html-b5e42919.js" as="script"><link rel="prefetch" href="/notebook/assets/Swagger.html-163a1cd4.js" as="script"><link rel="prefetch" href="/notebook/assets/Web静态资源处理.html-4eacdfc4.js" as="script"><link rel="prefetch" href="/notebook/assets/thymeleaf.html-6fe41ccd.js" as="script"><link rel="prefetch" href="/notebook/assets/分布式.html-e4e536cf.js" as="script"><link rel="prefetch" href="/notebook/assets/异步、定时、邮件任务.html-e7fe2c2a.js" as="script"><link rel="prefetch" href="/notebook/assets/整合mybatis.html-dc407a1d.js" as="script"><link rel="prefetch" href="/notebook/assets/第一天.html-5b45ced2.js" as="script"><link rel="prefetch" href="/notebook/assets/第二天.html-5dda30f7.js" as="script"><link rel="prefetch" href="/notebook/assets/ES6补充.html-668234e1.js" as="script"><link rel="prefetch" href="/notebook/assets/Vue基础.html-91a7a31f.js" as="script"><link rel="prefetch" href="/notebook/assets/Vue项目部署.html-1c3eb1d8.js" as="script"><link rel="prefetch" href="/notebook/assets/threejs的使用.html-350e2816.js" as="script"><link rel="prefetch" href="/notebook/assets/单文件组件.html-5bc8b3be.js" as="script"><link rel="prefetch" href="/notebook/assets/脚手架.html-ca084d7d.js" as="script"><link rel="prefetch" href="/notebook/assets/页面title.html-979513ef.js" as="script"><link rel="prefetch" href="/notebook/assets/Bitmaps.html-b806a27f.js" as="script"><link rel="prefetch" href="/notebook/assets/Hyperloglog.html-ccbee38e.js" as="script"><link rel="prefetch" href="/notebook/assets/Redis的基本事务操作.html-9277f0e7.js" as="script"><link rel="prefetch" href="/notebook/assets/redis-benchmark性能测试.html-0c4eea37.js" as="script"><link rel="prefetch" href="/notebook/assets/基本知识点.html-2442010c.js" as="script"><link rel="prefetch" href="/notebook/assets/安装.html-44dabf62.js" as="script"><link rel="prefetch" href="/notebook/assets/集成Redis.html-e0fe8e54.js" as="script"><link rel="prefetch" href="/notebook/assets/Center_pooling.html-7434a9ae.js" as="script"><link rel="prefetch" href="/notebook/assets/Focal_Loss.html-c736ff86.js" as="script"><link rel="prefetch" href="/notebook/assets/MAE.html-4785c79d.js" as="script"><link rel="prefetch" href="/notebook/assets/NMS非极大值抑制.html-8694503d.js" as="script"><link rel="prefetch" href="/notebook/assets/corner_pooling.html-1e3cd173.js" as="script"><link rel="prefetch" href="/notebook/assets/smooth_L1_loss.html-f9ed61b5.js" as="script"><link rel="prefetch" href="/notebook/assets/CMU motion capture.html-e2773175.js" as="script"><link rel="prefetch" href="/notebook/assets/Human3.6M.html-1f34af7d.js" as="script"><link rel="prefetch" href="/notebook/assets/CLIP.html-03a7b405.js" as="script"><link rel="prefetch" href="/notebook/assets/Faster_RCNN.html-4c33a2fb.js" as="script"><link rel="prefetch" href="/notebook/assets/MoCo.html-99e46914.js" as="script"><link rel="prefetch" href="/notebook/assets/RoI池化层.html-ab6c63f5.js" as="script"><link rel="prefetch" href="/notebook/assets/TCN.html-81df50ac.js" as="script"><link rel="prefetch" href="/notebook/assets/VAE.html-ff059abf.js" as="script"><link rel="prefetch" href="/notebook/assets/soft-argmax.html-238b0d93.js" as="script"><link rel="prefetch" href="/notebook/assets/元学习.html-8e751b92.js" as="script"><link rel="prefetch" href="/notebook/assets/反投影（back-projecting）.html-3e3d03e7.js" as="script"><link rel="prefetch" href="/notebook/assets/对比学习.html-834d7bdc.js" as="script"><link rel="prefetch" href="/notebook/assets/训练技巧.html-856fd83a.js" as="script"><link rel="prefetch" href="/notebook/assets/选择性搜索.html-30e2c070.js" as="script"><link rel="prefetch" href="/notebook/assets/A convnet for the 2020s.html-901e6ef5.js" as="script"><link rel="prefetch" href="/notebook/assets/Co-occurrence Feature Learning from Skeleton Data.html-0e29fe70.js" as="script"><link rel="prefetch" href="/notebook/assets/DETRs Beat YOLOs on Real-time Object Detection.html-2021478c.js" as="script"><link rel="prefetch" href="/notebook/assets/Disentangling Identity and Pose for Facial Expression Recognition.html-23fc93ca.js" as="script"><link rel="prefetch" href="/notebook/assets/Dynamic Multiscale Graph Neural Networks for 3D Skeleton-Based Human Motion Prediction.html-65f68e7a.js" as="script"><link rel="prefetch" href="/notebook/assets/EVOLVING REINFORCEMENT LEARNING ALGORITHMS.html-847e3006.js" as="script"><link rel="prefetch" href="/notebook/assets/Lite DETR An Interleaved Multi-Scale Encoder for Efficient DETR.html-ed1f2a21.js" as="script"><link rel="prefetch" href="/notebook/assets/Long-term Human Motion Prediction with Scene Context.html-a12c42cf.js" as="script"><link rel="prefetch" href="/notebook/assets/Real-time 2D Multi-Person Pose Estimation on CPU.html-00850641.js" as="script"><link rel="prefetch" href="/notebook/assets/SSHFD Single Shot Human Fall Detection with Occlud.html-fe4ba61a.js" as="script"><link rel="prefetch" href="/notebook/assets/Scaling Up Your Kernels to 31x31 Revisiting Large.html-9a8b9fcd.js" as="script"><link rel="prefetch" href="/notebook/assets/SkeleMotion A New Representation of Skeleton Joint.html-f48d52c4.js" as="script"><link rel="prefetch" href="/notebook/assets/Spatial Temporal Graph Convolutional Networks for.html-f7e0dab4.js" as="script"><link rel="prefetch" href="/notebook/assets/TSM Temporal Shift Module for Efficient Video Unde.html-e0f7bbc5.js" as="script"><link rel="prefetch" href="/notebook/assets/Towards Automated and Marker-less Parkinson Diseas.html-a172ea8a.js" as="script"><link rel="prefetch" href="/notebook/assets/Two-Stream Convolutional Networks for Action Recog.html-7320af12.js" as="script"><link rel="prefetch" href="/notebook/assets/🌟LoGoNet_ Towards Accurate 3D Object Detection with Local-to-Global Cross-Modal Fusion.html-164878e0.js" as="script"><link rel="prefetch" href="/notebook/assets/概述.html-82ca03b1.js" as="script"><link rel="prefetch" href="/notebook/assets/时间序列模型.html-5eb2c87a.js" as="script"><link rel="prefetch" href="/notebook/assets/线性代数.html-9d56d107.js" as="script"><link rel="prefetch" href="/notebook/assets/存储器.html-f1724edc.js" as="script"><link rel="prefetch" href="/notebook/assets/系统总线.html-65a88c47.js" as="script"><link rel="prefetch" href="/notebook/assets/计算机基本组成.html-a6e6ab30.js" as="script"><link rel="prefetch" href="/notebook/assets/计算机的运算方法.html-06419318.js" as="script"><link rel="prefetch" href="/notebook/assets/输入输出系统.html-70cdf8b9.js" as="script"><link rel="prefetch" href="/notebook/assets/高速缓存存储器.html-c82d9391.js" as="script"><link rel="prefetch" href="/notebook/assets/JDBC.html-db742cd3.js" as="script"><link rel="prefetch" href="/notebook/assets/JSP.html-5694e9e1.js" as="script"><link rel="prefetch" href="/notebook/assets/Servlet.html-3f528292.js" as="script"><link rel="prefetch" href="/notebook/assets/Tomcat.html-8f5df9f1.js" as="script"><link rel="prefetch" href="/notebook/assets/监听器.html-6dce5d6f.js" as="script"><link rel="prefetch" href="/notebook/assets/缓存.html-bde43d43.js" as="script"><link rel="prefetch" href="/notebook/assets/过滤器.html-f022c23a.js" as="script"><link rel="prefetch" href="/notebook/assets/IO.html-8ff1d015.js" as="script"><link rel="prefetch" href="/notebook/assets/反射.html-74e5bb6a.js" as="script"><link rel="prefetch" href="/notebook/assets/容器.html-eeb7b705.js" as="script"><link rel="prefetch" href="/notebook/assets/泛型.html-909eb974.js" as="script"><link rel="prefetch" href="/notebook/assets/注解.html-5e39bee7.js" as="script"><link rel="prefetch" href="/notebook/assets/概述.html-61749c53.js" as="script"><link rel="prefetch" href="/notebook/assets/CRUD.html-f0ca6cf6.js" as="script"><link rel="prefetch" href="/notebook/assets/ResultMap结果集映射.html-d3230fa0.js" as="script"><link rel="prefetch" href="/notebook/assets/lombok.html-27a4c600.js" as="script"><link rel="prefetch" href="/notebook/assets/一对多.html-3397922b.js" as="script"><link rel="prefetch" href="/notebook/assets/分页.html-9eec0880.js" as="script"><link rel="prefetch" href="/notebook/assets/动态SQL.html-8d138e1b.js" as="script"><link rel="prefetch" href="/notebook/assets/日志.html-3d803ee9.js" as="script"><link rel="prefetch" href="/notebook/assets/概念.html-78ac39d6.js" as="script"><link rel="prefetch" href="/notebook/assets/注解开发.html-88569228.js" as="script"><link rel="prefetch" href="/notebook/assets/生命周期和作用域.html-c5d10be9.js" as="script"><link rel="prefetch" href="/notebook/assets/第一个Mybatis程序.html-e9c69dc2.js" as="script"><link rel="prefetch" href="/notebook/assets/缓存.html-e08eaa72.js" as="script"><link rel="prefetch" href="/notebook/assets/配置解析.html-4ecd170f.js" as="script"><link rel="prefetch" href="/notebook/assets/404.html-4e6aa668.js" as="script"><link rel="prefetch" href="/notebook/assets/style-e9220a04.js" as="script"><link rel="prefetch" href="/notebook/assets/docsearch-1d421ddb.js" as="script"><link rel="prefetch" href="/notebook/assets/index-ade63bb5.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container"><!--[--><header class="navbar"><div class="toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a href="/notebook/" class=""><!----><span class="site-name">记录</span></a></span><div class="navbar-items-wrapper" style=""><!--[--><!--]--><nav class="navbar-items can-hide"><!--[--><div class="navbar-item"><a href="/notebook/知识/数据结构.html" class="" aria-label="测试"><!--[--><!--]--> 测试 <!--[--><!--]--></a></div><div class="navbar-item"><a class="external-link" href="https://github.com/dtenghao?tab=repositories" rel="noopener noreferrer" target="_blank" aria-label="GitHub"><!--[--><!--]--> GitHub <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></div><!--]--></nav><!--[--><!--]--><button class="toggle-color-mode-button" title="toggle color mode"><svg style="" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg style="display:none;" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!--[--><div id="docsearch-container" style="display:none;"></div><div><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"><svg width="15" height="15" class="DocSearch-Control-Key-Icon"><path d="M4.505 4.496h2M5.505 5.496v5M8.216 4.496l.055 5.993M10 7.5c.333.333.5.667.5 1v2M12.326 4.5v5.996M8.384 4.496c1.674 0 2.116 0 2.116 1.5s-.442 1.5-2.116 1.5M3.205 9.303c-.09.448-.277 1.21-1.241 1.203C1 10.5.5 9.513.5 8V7c0-1.57.5-2.5 1.464-2.494.964.006 1.134.598 1.24 1.342M12.553 10.5h1.953" stroke-width="1.2" stroke="currentColor" fill="none" stroke-linecap="square"></path></svg></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div></header><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar"><nav class="navbar-items"><!--[--><div class="navbar-item"><a href="/notebook/知识/数据结构.html" class="" aria-label="测试"><!--[--><!--]--> 测试 <!--[--><!--]--></a></div><div class="navbar-item"><a class="external-link" href="https://github.com/dtenghao?tab=repositories" rel="noopener noreferrer" target="_blank" aria-label="GitHub"><!--[--><!--]--> GitHub <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="sidebar-items"><!--[--><li><p tabindex="0" class="sidebar-item sidebar-heading">目录 <!----></p><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/notebook/%E7%A7%91%E7%A0%94/%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/Adapters.html#adapters" class="router-link-active router-link-exact-active sidebar-item" aria-label="Adapters"><!--[--><!--]--> Adapters <!--[--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="page"><!--[--><!--]--><div class="theme-default-content"><!--[--><!--]--><div><h1 id="adapters" tabindex="-1"><a class="header-anchor" href="#adapters" aria-hidden="true">#</a> Adapters</h1><p>Time: April 14, 2023 9:24 AM</p><blockquote><p><strong>Gao, Peng, et al. &quot;Clip-adapter: Better vision-language models with feature adapters.&quot; <em>arXiv preprint arXiv:2110.04544</em> (2021).</strong></p></blockquote><ul><li>在transformer中每个layers后面加一个Adapter（Houlsby et al., 2019）</li><li>将clip的zero-shot的模型权重和常规finetune（只finetune了image branch）的模型权重进行线性组合（Wortsman et al., 2021）</li></ul><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled.png" alt="Untitled"></p><p>融合参数是可学习的</p><p><strong>实验设置：</strong></p><ul><li>训练在1，2，4，8，16shot，在整个测试集上测试（oxford_flowers101）</li><li>使用一块A100</li><li>batchsize为32</li><li>学习率为$1\times10^{-5}$（$1\times10^{-4}$）</li><li>隐藏层大小为256</li><li>prompt设置为a photo of a {CLASS}</li><li>visual encoder使用ResNet50，textual使用BERT</li></ul><p><strong>代码细节：</strong></p><ul><li>只对visual encoder或者textual encoder进行Adapter，二选一</li></ul><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">2</span> python main.py <span class="token parameter variable">--config</span> configs/oxford_flowers.yaml
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 1.png" alt="Untitled"></p><p>与论文结果差了2-3个点</p><blockquote><p><strong>Zhang, Renrui, et al. &quot;Tip-adapter: Training-free adaption of clip for few-shot classification.&quot; <em>Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXV</em>. Cham: Springer Nature Switzerland, 2022.</strong></p></blockquote><p>过去的工作中，CoOp，CLIP-Adapter针对少量样本，对模型进行微调，实现性能的提升；Zero-shot CLIP，Linearprobe CLIP则不需要训练，也能达到很好的效果。</p><p>作者希望模型能利用到CLIP在Zero-Shot上的强大能力和Few-Shot上的优越性能。本文模型能够在不训练的情况下，表现与微调过的CoOp，CLIP-Adapter相当。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 2.png" alt="Untitled"></p><p>Cache Model，键值缓存存储了从Few-Shot训练集中提取的所有新知识，用于更新预训练CLIP中编码的先验知识。</p><p>$$ \varphi(x)=\exp (-\beta(1-x)) $$</p><p>$\beta$是超参数，用于控制相似度的趋势。exp保证相似度不为负。</p><p>最后再与CLIP的pre-train的知识进行结合，得到最终结果。前者自适应地总结来自Few-Shot训练数据集的信息，后者保留先验知识。</p><p>这两项由权值α平衡。根据经验，如果预训练的和下游的Few-Shot任务之间的domain差距很大，α被设置为大，因为需要从Few-Shot集中获得更多的知识，否则α被设置为小。</p><p>但Few-Shot的数量增加，TIP会逐渐落后于finetune模型。因此将Cache Keys作为可学习的参数，还可以显著的提升模型性能，而且它只需要20 epochs在ImageNet上，而CoOp和CLIP-Adapter需要200 epochs。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 3.png" alt="Untitled"></p><p><strong>实验设置：</strong></p><ul><li>使用1, 2, 4, 8, 16 few-shot</li><li>visual encoder使用ResNet50，text encoder使用transformer</li><li>batchSize为256（内存不够，设置为32）</li><li>学习率0.001</li><li>AdamW优化器</li><li>训练20个epoch</li><li>对图片进行10次数据增强，取平均特征</li></ul><p><strong>代码：</strong></p><p><a href="https://github.com/gaopengcuhk/Tip-Adapter" target="_blank" rel="noopener noreferrer">https://github.com/gaopengcuhk/Tip-Adapter<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">git</span> clone https://github.com/gaopengcuhk/Tip-Adapter.git
<span class="token builtin class-name">cd</span> Tip-Adapter

conda create <span class="token parameter variable">-n</span> tip_adapter <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.7</span>
conda activate tip_adapter

pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt

<span class="token comment"># Install the according versions of torch and torchvision</span>
conda <span class="token function">install</span> pytorch torchvision
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>datasets文件夹是关于如何处理数据集，并返回处理好后的数据</p><p>config中存放着训练的基本设置，包括数据集的位置，backbone的选择，一些超参数的设置</p><p>在根目录创建一个data文件夹，将下载好的数据集放到文件夹中，对于ImageNet数据集，文件结构应该为：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>imagenet/
<span class="token operator">|</span>–– images/
<span class="token operator">|</span>   <span class="token operator">|</span>–– train/ <span class="token comment"># contains 1,000 folders like n01440764, n01443537, etc.</span>
<span class="token operator">|</span>   <span class="token operator">|</span>–– val/
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>将classnames.txt放置在imagenet/目录下。如果要使用其他数据集，可以参考：</p><p><a href="https://github.com/gaopengcuhk/Tip-Adapter/blob/main/DATASET.md" target="_blank" rel="noopener noreferrer">Tip-Adapter/DATASET.md at main · gaopengcuhk/Tip-Adapter<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 4.png" alt="Untitled"></p><p>运行代码时，对于ImageNet数据集，指定使用第一个GPU，配置文件为imagenet.yaml</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python main_imagenet.py <span class="token parameter variable">--config</span> configs/imagenet.yaml
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>其他十种数据集如oxford_flowers，多了个搜索最佳$\alpha$和$\beta$的步骤：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">2</span> python main.py <span class="token parameter variable">--config</span> configs/oxford_flowers.yaml
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ul><li><p>加载CLIP模型</p></li><li><p>准备数据集（cache Model，训练集，测试集，验证集），16-shot，</p></li><li><p>计算text features</p></li><li><p>对cache Model数据集进行10次数据增强，特征取这10次的平均，构建cache Model</p></li><li><p>在验证集上搜索最佳$\alpha$和$\beta$</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 5.png" alt="Untitled"></p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 6.png" alt="Untitled"></p></li><li><p>在测试上评估模型</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 7.png" alt="Untitled"></p></li><li><p>使用训练集训练cache Model</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 8.png" alt="Untitled"></p></li><li><p>同样在验证集上搜索最佳$\alpha$和$\beta$，在测试集上评估模型</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 9.png" alt="Untitled"></p></li></ul><p>结果与论文基本一致</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 10.png" alt="Untitled"></p><blockquote><p>Pan, Junting, et al. &quot;ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning.&quot; <em>Advances in Neural Information Processing Systems</em> 35 (2022): 26462-26477.</p></blockquote><p>现有的方法都是关注相同模态中的迁移任务，预训练模型和下游任务是相同类型的任务，但是在某些特定模态（如视频理解）中受到限制，因为在这些领域，很难找到具有足够知识的强大预训练模型。</p><p>论文探讨了一种新颖的跨模态转移学习设置，叫作parameter-efficient image-to-video transfer learning，提出了一个新的Spatio-Temporal Adapter，这个模块可以使得一个没有时间知识的预训练图像模型能够以较小的（约8%）任务参数成本理解动态视频内容，与之前的工作相比，需要更新的参数减少了大约20倍。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 11.png" alt="Untitled"></p><p>在NLP中，Adapter通常结构如下：</p><p>$$ \text{Addapter}(\mathbf X)=\mathbf X+f(\mathbf X\mathbf W_{down})\mathbf W_{up} $$</p><p>论文目标是将Adapter的成功从NLP推广到计算机视觉，特别是前面讨论的图像-视频迁移学习问题。</p><p>$$ \text{ST-Adapter}(\mathbf{X})=\mathbf{X}+f\Big(\text{DWConv3D}(\mathbf{X}\mathbf{W}<em>{down})\Big)\mathbf{W}</em>{up} $$</p><p>因为channel维度小（128），因此depth-wise convolution可以非常的高效，与NLP Adapter差距不大。</p><p>之前在NLP领域已经讨论了Adapter应该放在哪里，一开始在每一个encoder中都部署了两个Adapter模块，一个跟在多头自注意力（MHSA）之后，另一个跟在前馈神经网络（FFN）之后。然而，后面有人提出仅在 FFN 之后添加一个Adapter模块就足够了。类似地，ST-Adapter（SpatioTemporal Adapter）也可以被集成在不同的位置。根据实验证据，作者发现在每个 Transformer 块的 MHSA 之前放置一个 ST-Adapter 就能达到不错的性能。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 12.png" alt="Untitled"></p><blockquote><p><strong>Chen, Zhe, et al. &quot;Vision transformer adapter for dense predictions.&quot; <em>arXiv preprint arXiv:2205.08534</em> (2022).</strong></p></blockquote><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 13.png" alt="Untitled"></p><p>与最近发展的将视觉特定归纳偏置引入模型的Swin等相比，普通ViT在密集预测上的性能较差，原因在于较弱的先验假设。为解决这一问题，作者提出了ViT-Adapter，使普通ViT能够在性能上与视觉特定的Transformer相媲美。</p><p><strong>为什么要在plain ViT上改</strong>：trnsformer对于输入没有assumption，无论是patch embedding、3D patch embedding还是token embedding，plain ViT都可以将他们作为输入，因此作者认为plain ViT在多模态训练上有非常好的优势。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 14.png" alt="Untitled"></p><ul><li>ViT将输入图片分为16$\times$ 16大小的多个patch，然后每个patch转化为D维embedding，经过L个encoder层。</li><li>首先将图片输入到空间先验模块，得到1/8，1/16，1/32分辨率，维度为D的三张特征图，再将三张特征图进行flattened和concatenated，设置N个interaction模块（通常N=4），将L个transformer encoders均匀地划分到N个interaction模块中。在interaction模块前后都进行空间先验特征的inject和extract层次特征，经过N个模块后，就可以获得高质量的多尺度特征，从而进行相应的下游任务。</li></ul><p><strong>SPATIAL PRIOR MODULE</strong></p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 15.png" alt="Untitled"></p><p>Stem的思想是从ResNet借鉴过来的，包含了三个3$\times$3，步长为2的卷积，channel都进行翻倍，以及一个最大池化层，最后通过一些1$\times$1的卷积，来使得最后的channel数为D，这样就得到了三个大小为1/8, 1/16和1/32的特征图，最后对它们进行flatten 和 concatenate，最后得到了</p><p>$$ \mathcal{F}_{\mathrm{sp}}^{1} \in \mathbb{R}^{\left(\frac{H W}{8^{2}}+\frac{H W}{16^{2}}+\frac{H W}{32^{2}}\right) \times D} $$</p><p><strong>Spatial Feature Injector</strong></p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 16.png" alt="Untitled"></p><p>$$ \mathcal{F}_{\text{rit}}^i \in \mathbb{R}^{\frac{HW}{16^2}\times D} $$</p><p>$$ \mathcal{F}_{\text{sp}}^{i}\in\mathbb{R}^{(\frac{HW}{8^{2}}+\frac{HW}{16^{2}}+\frac{HH}{32^{2}})\times D} $$</p><p>$$ \hat{\mathcal{F}}^i_{\text{vit}}=\mathcal{F}^i_{\text{vit}}+\gamma^i\text{Attention}(\text{norm}(\mathcal{F}^i_\text{vit}),\text{norm}(\mathcal F^i_{\text{sp}})) $$</p><p>$\gamma^i \in \mathbb{R}^D$是可学习向量，初始化为0，</p><p><strong>Multi-Scale Feature Extractor</strong></p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 17.png" alt="Untitled"></p><p>将得到的$\hat{\mathcal{F}}^i_{\text{vit}}$经过ViT中第i个interaction模块后，得到$\mathcal{F}_{\text{vit}}^{i+1}$，然后继续使用cross-attention层和FFN来提取多尺度特征</p><p>$$ \begin{matrix}\mathcal{F}<em>{\text{sp}}^{i+1}=\hat{\mathcal{F}}</em>{\text{sp}}^i+\mathrm{FFN}(\text{norm}(\hat{\mathcal{F}}^i_\text{sp})),\ \hat{\mathcal{F}}<em>\text{sp}^i=\mathcal{F}</em>\text{sp}^{i}+\mathrm{Attention}(\text{norm}(\mathcal{F}^i_\text{sp}),\hom(\mathcal{F}^{i+1})),\end{matrix} $$</p><p>作者构建了四种不同大小的ViT-Adapter，分别基于ViT-T, ViT-S, ViT-B, 和 ViT-L。这些模型的Adapters参数数量分别为2.5M（百万），5.8M，14.0M，和23.7M。</p><p>在方法中，默认使用可变形注意力（deformable attention，Zhu et al., 2020）作为稀疏注意力。其中，采样点的数量固定为4，不同模型的注意力头数分别设置为6, 6, 12, 和 16。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 18.png" alt="Untitled"></p><p><a href="https://github.com/czczup/ViT-Adapter" target="_blank" rel="noopener noreferrer">https://github.com/czczup/ViT-Adapter<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>实验设置：</p><ul><li>代码基于MMDetection，数据集为COCO2014</li><li>预训练在ImageNet-1K的DeiT作为backbone</li><li>使用四种主流检测头：Mask R-CNN , Cascade Mask R-CNN , ATSS and GFL</li><li>36 epoch</li><li>batch size为16</li><li>AdamW优化器，学习率$1\times10^{-4}$，衰减0.05</li></ul><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>pip <span class="token function">install</span> torch torchvision torchaudio
pip <span class="token function">install</span> openmim
mim <span class="token function">install</span> mmcv-full<span class="token operator">==</span><span class="token number">1.5</span>.0
mim <span class="token function">install</span> <span class="token assign-left variable">mmdet</span><span class="token operator">==</span><span class="token number">2.22</span>.0
pip <span class="token function">install</span> <span class="token assign-left variable">timm</span><span class="token operator">==</span><span class="token number">0.4</span>.12
pip <span class="token function">install</span> instaboostfast
<span class="token builtin class-name">cd</span> ops <span class="token operator">&amp;</span> <span class="token function">sh</span> make.sh
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>代码细节：</p><ul><li>extractor层的FFN使用的是深度可分离卷积(DW)，它会对输入的x重新进行划分为三个尺度，分别对他们进行卷积操作，最后在合并</li></ul><p>使用了MMDetection工具，只需要定义model(backbone,neck)，数据集位置，一些环境设置(GPU数量，checkpoint文件位置)，就可以进行训练。</p><p>训练：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 配置文件为configs/mask_rcnn/mask_rcnn_deit_adapter_tiny_fpn_3x_coco.py</span>
<span class="token comment"># GPU数量为2</span>
<span class="token function">sh</span> dist_train.sh configs/mask_rcnn/mask_rcnn_deit_adapter_tiny_fpn_3x_coco.py <span class="token number">2</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 19.png" alt="Untitled"></p><p>大概要训练一两天。</p><p>测试：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">sh</span> dist_test.sh configs/mask_rcnn/mask_rcnn_deit_adapter_base_fpn_3x_coco.py /data/dth/ViT_Adapter/pretrained/deit_base_patch16_224-b5f2ef4d.pth <span class="token number">2</span> <span class="token parameter variable">--eval</span> bbox segm
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><blockquote><p>Yang, Taojiannan, et al. &quot;AIM: Adapting Image Models for Efficient Video Action Recognition.&quot; <em>arXiv preprint arXiv:2302.03024</em> (2023).</p></blockquote><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 20.png" alt="Untitled"></p><p>实验设置：</p><ul><li>使用Diving-48数据集</li><li>使用CLIP预训练模型，ViT-B/16</li><li>AdamW优化器，学习率3e-4，衰减0.05</li><li>训练 50 epoch，batch size为64</li></ul><p>代码细节：</p><ul><li>S_Adapter使用了残差连接，而MLP_Adapter和T_Adapter没有使用残差连接</li></ul><p>使用了MMaction框架，只需要定义model，数据集位置，一些环境设置(GPU数量，checkpoint文件位置)，就可以进行训练。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">bash</span> tools/dist_train.sh configs/recognition/vit/vitclip_base_diving48.py <span class="token number">2</span> --test-last <span class="token parameter variable">--validate</span> --cfg-options <span class="token assign-left variable">work_dir</span><span class="token operator">=</span>./work_dir --resume-from /data/dth/diving48/vit_b_clip_32frame_diving48.pth
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 21.png" alt="Untitled"></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 使用官方pth</span>
<span class="token function">bash</span> tools/dist_test.sh configs/recognition/vit/vitclip_base_diving48.py /data/dth/diving48/vit_b_clip_32frame_diving48.pth <span class="token number">2</span> <span class="token parameter variable">--eval</span> top_k_accuracy
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 使用自己训练的pth</span>
<span class="token function">bash</span> tools/dist_test.sh configs/recognition/vit/vitclip_base_diving48.py /code/dth/adapt-image-models/work_dir/best_top1_acc_epoch_40.pth <span class="token number">2</span> <span class="token parameter variable">--eval</span> top_k_accuracy
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 22.png" alt="Untitled"></p><blockquote><p><strong>Mou, Chong, et al. &quot;T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.&quot; <em>arXiv preprint arXiv:2302.08453</em> (2023).</strong></p></blockquote><p>T2I模型需要精心设计文本prompts才能生成较好的图片，导致用户很难引导模型生成想象中的图片。作者认为，这并不意味着T2I模型不具备生成能力，只是文本不能提供准确的结构指导，也就是模型是有较强的生成能力的，但是缺乏指导它生成的能力（文本不能准确地完成）。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 23.png" alt="Untitled"></p><p>因此作者提出了T2I-Adapter，这是一个轻量级模型，可以用相对少量的数据来学习为预先训练好的T2I扩散模型(即SD[32])提供额外的指导。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 24.png" alt="Untitled"></p><p>Adapter包含了四个特征提取块和三个下采样块，最终将四个多尺度特征图与SD模型中每个UNet denoiser相加。这样adapters就可以提取不同类型condition的指导特征，与输入的文本特征一起参与到SD的图片生成中了。</p><p>$$ \mathbf{F}<em>c=\sum</em>{k=1}^K\omega_k\mathcal{F}^k_{AD}(\mathbf{C}_k), $$</p><p>因此可以使用草图、mask、keypose、语义分割图、深度图等condition，指导T2I模型生成图片。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 25.png" alt="Untitled"></p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 26.png" alt="Untitled"></p><p>这些论文探讨了将Adapter应用于不同模型和任务的各种方法，包括视觉-语言模型、少样本分类、图像到视频迁移学习、密集预测以及文本到图像扩散模型。Adapter为将预训练模型的知识迁移到新任务提供了一种高效的方式，通过最小化参数更新来实现更好的性能和对模型输出的细粒度控制。</p><ol><li>在CLIP的输出后加上Adapter，同时使用残差连接线性组合预训练模型和微调模型的权重，以改进视觉-语言任务。</li><li>Tip-Adapter在不经过训练的情况下实现了与微调过的CoOp和CLIP-Adapter模型相当的性能，充分利用了CLIP的零样本能力和少样本性能。如果对Cache Model中的key进行训练，还可进一步大幅提高模型性能。</li><li>ST-Adapter通过时空Adapter模块实现了高效的图像到视频迁移学习，显著减少了参数更新所需的数量。</li><li>将Adapter用于plain ViTs，通过加入空间先验模块和多尺度特征提取，提高了模型的性能，使其可以与Swin等对视觉先验信息有特殊设计的模型表现相当。同时还可用于各种密集预测任务</li><li>T2i-Adapter旨在为文本到图像扩散模型提供更可控的能力，使用户能够更好地根据文本、草图、mask等提示引导模型的输出。</li></ol><p><a href="https://github.com/taoyang1122/adapt-image-models" target="_blank" rel="noopener noreferrer">https://github.com/taoyang1122/adapt-image-models<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><blockquote><p>He, Junxian, et al. &quot;Towards a unified view of parameter-efficient transfer learning.&quot; <em>arXiv preprint arXiv:2110.04366</em> (2021).</p></blockquote><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 27.png" alt="Untitled"></p><p><strong>Adapters：将特征向量经过一个下采样、非线性激活层、上采样后，再残差连接</strong></p><p>$$ h\leftarrow h+f(hW_{\text{down}})W_{\text{up}} $$</p><p><strong>Prefix Tuning：将原始的key和value前加上可学习的prompt</strong></p><p>$$ \text{head}_i=\text{Atn}(\pmb{x}\pmb{W}_q^{(i)},\text{const}(\pmb{P}_k^{(i)},\pmb{C}\pmb{W}_k^{(i)}),\pmb{\text{const}}(\pmb{P}_v^{(i)},\boldsymbol{CW}_v^{(i)})) $$</p><p><strong>LoRA：将可训练的低秩矩阵注入到transformer中，以近似权值更新</strong></p><p>$$ h\leftarrow h+s\cdot x\boldsymbol{W}<em>{\text{down}}W</em>{\text{up}}, $$</p><p>对于prefix tunning，对公式进行拆开：</p><p>$$ (1-\lambda(\boldsymbol x))\text{Atn}(\boldsymbol x\boldsymbol W_q,\boldsymbol CW_k,\boldsymbol CW_v)+\lambda(\boldsymbol x)\text{A}\text{tn}(\boldsymbol x\textbf{W}_q,\boldsymbol P_k,\boldsymbol P_v) $$</p><p>令$\textbf{W}_1\textbf{=}W_q\textbf{P}_k^\top,\textbf{W}_2\textbf{=}P_v$</p><p>$$ \pmb{h}\leftarrow(1-\lambda(\pmb{x}))h+\lambda(\pmb{x})f(\pmb{x}\pmb{W}_1)\pmb{W}_2 $$</p><p>因此可以把三个方法归纳为：</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 28.png" alt="Untitled"></p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/18/Untitled 29.png" alt="Untitled"></p><p><strong>结论：</strong></p><ul><li>并联方式</li><li>将adapter中0.1%的参数放在attn，剩余的参数放在FFN（通常adapter放在FFN后效果更好，但当参数量较少，放在attn后提升更大）</li><li>使用$\boldsymbol{h}\leftarrow h+s\cdot\Delta h$结构</li></ul></div><!--[--><!--]--></div><footer class="page-meta"><div class="meta-item edit-link"><a class="external-link meta-item-label" href="https://github.com/dtenghao?tab=repositories/edit/main/科研/方法概述/Adapters.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page"><!--[--><!--]--> Edit this page <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></div><!----><!----></footer><!----><!--[--><!--]--></main><!--]--></div><!----><!--]--></div>
    <script type="module" src="/notebook/assets/app-29aba0ee.js" defer></script>
  </body>
</html>
