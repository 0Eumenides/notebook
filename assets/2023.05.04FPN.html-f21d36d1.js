import{_ as n,M as a,p as s,q as m,R as e,N as i,V as r,t,a1 as p}from"./framework-e03faf0e.js";const l={},c=p('<h1 id="detr" tabindex="-1"><a class="header-anchor" href="#detr" aria-hidden="true">#</a> DETR</h1><blockquote><p>Carion, Nicolas, et al. &quot;End-to-end object detection with transformers.&quot; <em>Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16</em>. Springer International Publishing, 2020.</p></blockquote><p>从最早的faster-rcnn，提出了各种proposal方法，到YOLO采用one-stage，但也是基于anchor这种方法。</p><p>得到预测框后也都需要用NMS对结果进行过滤。而DETR都不需要，DETR减少了大量手工设计的组件，极大的简化了目标检测的流程，实现了端到端的目标检测</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/04/image-20230504074321868.png" alt="image-20230504074321868"></p><ul><li><p>图片经过Backbone得到特征，加上位置编码输入到transformer编码器中，进一步得到更优秀的特征</p></li><li><p>解码器用0初始化固定数量的object queries（需要大于图片中的目标数，论文设置为100），加上位置编码将编码器得到的特征作为key和value输入到解码器中</p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/04/image-20230504080900683.png" alt="image-20230504080900683" style="zoom:50%;"></li><li><p>将解码器得到的特征输入到FNN中，得到每个queries最终的bbox和cls</p></li></ul><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/04/image-20230504075424887.png" alt="image-20230504075424887"></p><p>经过编码器之后，可以很好的分离每个目标</p><p>解码器每层的输出结果都进行loss的计算</p><p>将预测出来的100个边界框与ground truth进行二分图匹配，使得损失最小，box损失是GIoU损失和L1损失 $$ \\hat{\\sigma}=\\underset{\\sigma\\in\\mathbb{S}<em>N}{\\operatorname{arg}\\operatorname*{min}}\\sum\\limits</em>{i}^\\mathbb{N}\\mathcal{L}<em>{\\mathrm{match}}\\left(\\mathrm{y_i},\\hat{\\mathrm{y}}</em>{\\sigma(i)}\\right) $$</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/06/image-20230506225718044.png" alt="image-20230506225718044"></p><h1 id="deformable-detr" tabindex="-1"><a class="header-anchor" href="#deformable-detr" aria-hidden="true">#</a> Deformable DETR</h1><blockquote><p>Zhu, Xizhou, et al. &quot;Deformable detr: Deformable transformers for end-to-end object detection.&quot; <em>arXiv preprint arXiv:2010.04159</em> (2020).</p></blockquote><p>DETR消除了目标检任务中的<strong>手动设置anchor</strong>，但是存在<strong>收敛慢</strong>（注意力权重矩阵往往都很稀疏，DETR计算全部像素的注意力导致收敛速率慢）以及Transformer的自注意力造成的<strong>特征图分辨率不能太高</strong>的问题，这就导致了<strong>小目标检测性能很差</strong></p><p>Deformable DETR使用10x更少的训练轮次实现了更好的性能表现(特别是在小物体上)</p><p>==<strong>DeformAttention</strong>==</p><p>query不是和全局每个位置的所有key都计算注意力权重，而是对每个query，仅在全局位置中采样局部/部分位置的key，最后把这个局部/稀疏的注意力权重和局部value进行计算。 $$ \\text{Deform}\\text{Attn}(\\boldsymbol{z}<em>q,\\boldsymbol{p}<em>q,\\boldsymbol{x})=\\sum\\limits</em>{m=1}^M\\boldsymbol{W}<em>m\\big[\\sum\\limits</em>{k=1}^K A</em>{mqk}\\cdot\\boldsymbol{W}<em>m&#39;\\boldsymbol{x}(\\boldsymbol{p}<em>q+\\Delta\\boldsymbol{p}</em>{mqk})\\big] $$ $A</em>{mqk}$和$\\Delta\\boldsymbol{p}_{mqk}$都是${z}_q$通过全连接层得到</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/04/image-20230504105533376.png" alt="image-20230504105533376"></p><p>==<strong>MSDeformAttention</strong>==</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/04/image-20230504105124840.png" alt="image-20230504105124840"></p><p>$L$是特征层的数量，$\\phi_l(\\hat{p}_q)$则表示参考点在第$l$层特征图上的位置</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/04/image-20230504105820652.png" alt="image-20230504105820652"></p><p>==<strong>其它细节</strong>==</p><ol><li><p>Two-Stage Deformable DETR</p><p>编码器的输出经过一个三层的FFN来获得边界框回归，一个线性projection来获得边界框的二分类（前景和背景），边界框的预测公式如下： $$ \\hat{b}<em>i={\\sigma(\\Delta b</em>{ix}+\\sigma^{-1}(\\hat{p_{ix}})),\\sigma(\\Delta b_{iy}+\\sigma^{-1}(\\hat{p_{iy}})),\\sigma(\\Delta b_{iw}+\\sigma^{-1}(2^{l_i-1}s)),\\sigma(\\Delta h_{ih}+\\sigma^{-1}(2^{l_i-1}s))} $$ $i$表示第几层特征图，$\\hat{p_{i}}$则表示像素点，通过预测$\\Delta b_{i{x,y,w,h}}\\in\\mathbb{R}$来获得边界框，base 目标尺度$s$设置为0.05</p><p>最后用这些边界框中，分类得分top-k的区域建议坐标x,y来初始化bbox，宽高设置为0.1，实验发现宽高设置具有鲁棒性，不会影响模型表现</p><p>再通过初始化的bbox获取pos_embed，再通过linear和norm来得到解码器的Object Queries</p></li><li><p>迭代边界框优化</p><p>解码器的每一层预测$\\Delta b_{i{x,y,w,h}}\\in\\mathbb{R}$，不断的refine边界框</p><p>为了提高训练稳定性，每层的解码器梯度都不会回传到上一层</p></li></ol><h1 id="dino" tabindex="-1"><a class="header-anchor" href="#dino" aria-hidden="true">#</a> DINO</h1><blockquote><p>Zhang, Hao, et al. &quot;Dino: Detr with improved denoising anchor boxes for end-to-end object detection.&quot; <em>arXiv preprint arXiv:2203.03605</em> (2022).</p></blockquote><p>DETR把目标检测做成了一个<strong>set prediction</strong>的问题，并利用<strong>匈牙利匹配</strong>（Hungarian matching）算法来解决decoder输出的objects和ground-truth objects的匹配 。但是，匈牙利算法匹配的<strong>离散</strong>性和模型训练的<strong>随机</strong>性，导致ground-truth的匹配变成了一个<strong>动态</strong>的、<strong>不稳定</strong>的过程。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/04/image-20230504131331018.png" alt="image-20230504131331018"></p><ol><li><p>提出了一种**对比去噪( Contrastive DeNoising：CDN )**方法</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/04/image-20230504131510613.png" alt="image-20230504131510613"></p><p>作者发现DETR中的二分匹配在早期十分不稳定，这会导致优化目标不一致引起收敛缓慢的问题。因此使用一个denoising task直接把带有噪声的真实框输入到decoder中，作为一个shortcut来学习相对偏移，它跳过了匹配过程直接进行学习</p><p>对于每个GT框，都生成一个正样本（离GT框较近），一个负样本（离GT框较远，但又不是很远，难样本），以及一个随机的label，将label经过一个embedding变为content query，anchors则作为pos query</p><p>group就是生成正负样本的组数，将CDN和普通的query一起做decoder，但在自注意力中，要使用attention mask，使得普通的query不能看到CDN，CDN不同组之间也不能看到。最后普通的query做二分图匹配，CDN计算去噪损失</p><p>通过正样本来学习如何回归出GT框，利用负样本来学习如何区分背景</p></li><li><p>提出了<strong>Mixed Query Selection</strong></p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/04/image-20230504132145044.png" alt="image-20230504132145044"></p><ul><li>Static Queries：最初的DETR使用的方法，初始化content queries和position queries后，在训练中学习得到</li><li>Pure Query Selection：Deformable DETR从最后一个编码器层==使用分类分数top k的k个特征==作为先验来增强解码器查询，位置和内容查询都是由所选特征的线性变换生成的</li><li>Mixed Query Selection：本文使用的方法，只选择前 K 个编码器特征来初始化position queries(Anchors)，而保持content queries的可学习性，有助于模型利用更好的位置信息从编码器中汇集更全面的内容特征</li></ul></li></ol><p><strong>SOTA性能</strong>：在大模型上以相对较小的数据和模型（～<strong>1/10相比之前SwinV2</strong>）取得了最好的检测结果。在ResNet-50的标准setting下取得了<strong>51.3</strong> AP。</p><p><strong>Fast converging(收敛快)</strong>： 在标准的ResNet-50 setting下，使用 5 个尺度特征（5-scale）的 DINO 在 12 个 epoch 中达到 <strong>49.4</strong> AP，在 24 个 epoch 中达到 <strong>51.3</strong> AP。使用4个尺度特征（4-scale）的DINO达到了了类似的性能并可以以 <strong>23</strong> FPS 运行。</p><h1 id="lite-detr" tabindex="-1"><a class="header-anchor" href="#lite-detr" aria-hidden="true">#</a> Lite DETR</h1><blockquote><p>Li, Feng, et al. &quot;Lite DETR: An Interleaved Multi-Scale Encoder for Efficient DETR.&quot; <em>arXiv preprint arXiv:2303.07335</em> (2023).</p></blockquote>',33),g=e("h1",{id:"rt-detr",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#rt-detr","aria-hidden":"true"},"#"),t(" RT-DETR")],-1),u=e("blockquote",null,[e("p",null,[t('Lv, Wenyu, et al. "Detrs beat yolos on real-time object detection." '),e("em",null,"arXiv preprint arXiv:2304.08069"),t(" (2023).")])],-1);function d(h,b){const o=a("RouterLink");return s(),m("div",null,[c,e("p",null,[i(o,{to:"/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Lite%20DETR%20An%20Interleaved%20Multi-Scale%20Encoder%20for%20Efficient%20DETR.html"},{default:r(()=>[t("Lite DETR")]),_:1})]),g,u,e("p",null,[i(o,{to:"/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DETRs%20Beat%20YOLOs%20on%20Real-time%20Object%20Detection.html"},{default:r(()=>[t("RT-DETR")]),_:1})])])}const D=n(l,[["render",d],["__file","2023.05.04FPN.html.vue"]]);export{D as default};
