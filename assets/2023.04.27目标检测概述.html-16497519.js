import{_ as s,M as r,p as c,q as h,R as e,t as i,N as t,V as n,a1 as l}from"./framework-e03faf0e.js";const u={},p=l('<h1 id="yolo-系列" tabindex="-1"><a class="header-anchor" href="#yolo-系列" aria-hidden="true">#</a> YOLO 系列</h1><h2 id="yolo3" tabindex="-1"><a class="header-anchor" href="#yolo3" aria-hidden="true">#</a> YOLO3</h2><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/{year}/{month}/{day}/{filename}{.suffix}watermark%2Ctype_ZmFuZ3poZW5naGVpdGk%2Cshadow_10%2Ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNTgxMjI0%2Csize_16%2Ccolor_FFFFFF%2Ct_70.png" alt="img"> 最终每个特征像素会有85$\\times$3个channel，表示(anchor偏移量(XYWH)，置信度，类别(80个))$\\times$anchor数量（每个像素生成3个anchor）。</p><hr><p>$$ \\begin{gathered} Loss=\\lambda_{coord}\\sum\\limits_{i=0}^{S^2}\\sum\\limits_{j=0}^B I_{ij}^{obj}[(x_i-\\hat{x}<em>i^j)^2+(y_i-\\hat{y}<em>i^j)^2]+ \\ \\lambda</em>{c o o r d}\\sum</em>{i=0}^{S^{2}}\\sum_{j=0}^{B}I_{i j}^{c b j}[(\\sqrt{w_{i}^{j}}-\\sqrt{\\hat{w}<em>{i}^{j}})^{2}+(\\sqrt{h</em>{i}^{j}}-\\sqrt{\\hat{h}<em>{i}^{j}})^{2}]- \\ \\sum\\limits</em>{i=0}^{s^2}\\sum\\limits_{j=0}^B I_{ij}^{obj}[\\hat C_i^j\\log(C_i^j)+(1-\\hat C_i^j)\\log(1-C_i^j)]- \\ \\lambda_{n o o b j}\\sum_{i=0}^{\\hat{S}^{\\hat{2}}}\\sum_{j=0}^{\\hat{B}}I_{i j}^{n o o b j}[\\hat{C}<em>{i}^{j}\\log(C</em>{i}^{j})+(1-\\hat{C}<em>{i}^{j})\\log(1-C</em>{i}^{j})]- \\ \\sum\\limits_{i=0}^{5}I_{ij}^{obj}\\sum\\limits_{c\\in classes}([\\hat{P}_i^j\\log(P_i^j)+(1-\\hat{P}_i^j)\\log(1-P_i^j)] \\end{gathered} $$ 如果与每个目标的IoU最大的那个anchor，则作为正样本；小于某个阈值，则作为负样本。否则不参与训练。 损失包含正样本的坐标误差、置信度误差、类别误差和负样本的置信度误差。</p><hr>',6),m=l('<h2 id="yolo4" tabindex="-1"><a class="header-anchor" href="#yolo4" aria-hidden="true">#</a> YOLO4</h2><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/{year}/{month}/{day}/{filename}{.suffix}v2-88544afd1a5b01b17f53623a0fda01db_r_副本.jpg" alt="v2-88544afd1a5b01b17f53623a0fda01db_r_副本"></p><ol><li><p>Mosaic数据增强：采用了4张图片，<strong>随机缩放、随机裁剪、随机排布</strong>的方式进行拼接，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/27/v2-dddc368bc1c8ec6239d152c609774673_1440w.webp" alt="img"></p></li><li><p>使用了<strong>CSPDarknet53</strong>作为backbone，使用<strong>Mish</strong>激活函数，但后面的网络还是使用leaky_relu函数</p></li><li><p>使用了<strong>Dropblock</strong>，和常见网络中的Dropout功能类似，也是缓解过拟合的一种正则化方式。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/27/v2-8b9a2710b100dccd1ebc1fe500d5a7a1_1440w.webp" alt="img"></p><blockquote><p>中间Dropout的方式会随机的删减丢弃一些信息，但<strong>Dropblock的研究者</strong>认为，卷积层对于这种随机丢弃并不敏感，因为卷积层通常是三层连用：<strong>卷积+激活+池化层</strong>，池化层本身就是对相邻单元起作用。而且即使随机丢弃，卷积层仍然可以从相邻的激活单元学习到<strong>相同的信息</strong>。</p><p>因此，在全连接层上效果很好的Dropout在卷积层上<strong>效果并不好</strong>。</p><p>所以<strong>右图Dropblock的研究者</strong>则干脆整个局部区域进行删减丢弃。</p></blockquote></li><li><p>采用<strong>SPP模块</strong>的方式，比单纯的使用<strong>k*k最大池化</strong>的方式，更有效的增加主干特征的接收范围，显著的分离了最重要的上下文特征。</p></li><li><p>使用了更复杂的特征融合操作<strong>FPN+PAN</strong></p></li><li><p>在预测上，使用了<strong>CIoU_loss</strong>和<strong>DIOU_nms</strong></p></li></ol><h2 id="yolo5" tabindex="-1"><a class="header-anchor" href="#yolo5" aria-hidden="true">#</a> YOLO5</h2><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/27/v2-770a51ddf78b084affff948bb522b6c0_r.jpg" alt="img"></p><ol><li><p><strong>自适应锚框计算</strong>：前面的版本中，针对不同的数据集，都会有<strong>初始设定长宽的锚框</strong>。但Yolov5中将此功能嵌入到代码中，每次训练时，自适应的计算不同训练集中的最佳锚框值。</p></li><li><p><strong>自适应图片缩放</strong>：在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中，但因为很多图片的长宽比不同，因此缩放填充后，存在信息冗余，影响推理速度。因此yolo5在推理时，采用缩减黑边的方式，提高推理的速度。</p></li><li><p>加入了Focus操作</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/27/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LmQ5LqL5Y6f5ZGzfg==,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="img"></p></li><li><p>有四种网络结构<strong>Yolov5s、Yolov5m、Yolov5l、Yolov5x</strong>，网络逐渐加深。</p></li></ol><h2 id="yolo7" tabindex="-1"><a class="header-anchor" href="#yolo7" aria-hidden="true">#</a> YOLO7</h2><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/27/v2-d9a3d0afcf20225129f8e86716fc333f_1440w.webp" alt="img"> 官方版的YOLOv7相同体量下比YOLOv5精度更高，速度快120%（FPS），比 YOLOX 快180%（FPS），比 Dual-Swin-T 快1200%（FPS），比 ConvNext 快550%（FPS），比 SWIN-L快500%（FPS）。在5FPS到160FPS的范围内，无论是速度或是精度，YOLOv7都超过了目前已知的检测器，并且在GPU V100上进行测试， 精度为56.8% AP的模型可达到30 FPS（batch=1）以上的检测速率，与此同时，这是目前唯一一款在如此高精度下仍能超过30FPS的检测器。</p><h1 id="目标检测" tabindex="-1"><a class="header-anchor" href="#目标检测" aria-hidden="true">#</a> 目标检测</h1><p>一般的目标检测方法大致可以分为two-stage和one-stage两种方式：</p><ul><li>two-stage首先生成高质量的候选区域(RPN)，然后检测头(faster rcnn)将区域特征作为输入进行分类和定位；</li><li>one-stage则是直接对图片进行定位和分类，得益于proposal-free的设定，该方法效率高，但是通常精度落后于two-stage</li></ul><p>除了上面两种类别，近年来还涌现了一些==anchor-free的方法==，以及==query-based检测器==，将检测任务作为预测任务，展现出了很大的潜力。</p><h2 id="two-stage" tabindex="-1"><a class="header-anchor" href="#two-stage" aria-hidden="true">#</a> Two-stage</h2>',13),d=e("img",{src:"https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/23/image-20230423150140945.png",alt:"image-20230423150140945"},null,-1),g=e("li",null,[e("p",null,[i("Fast RCNN 相较于 RCNN，Fast RCNN 改进了特征提取过程。它将整个图像输入到 CNN 中进行特征提取，然后使用候选区域生成算法（如选择性搜索）和 "),e("a",{href:"../%E7%A7%91%E7%A0%94/%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/RoI%E6%B1%A0%E5%8C%96%E5%B1%82"},"RoI（Region of Interest）"),i("池化层在特征图上生成候选区域。最后，通过全连接层进行分类和边界框回归。这种方法避免了在每个候选区域上单独进行特征提取，从而大大提高了处理速度。")]),e("p",null,[e("img",{src:"https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/23/image-20230423150818834.png",alt:"image-20230423150818834"}),i(" ==不足==：Fast RCNN仍然选用选择性搜索算法来寻找感兴趣的区域，这一过程通常较慢，与RCNN不同的是，Fast RCNN处理一张图片大约需要2秒，但是在大型真实数据集上，这种速度仍然不够理想。")])],-1),_=e("img",{src:"https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/24/image-20230424190403628.png",alt:"image-20230424190403628",style:{zoom:"50%"}},null,-1),b=e("li",null,[e("p",null,"Cascade RCNN"),e("p",null,[e("img",{src:"https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/image-20230425085101085.png",alt:"image-20230425085101085"})]),e("ol",null,[e("li",null,[e("strong",null,"级联检测头"),i("：Cascade R-CNN 构建了一系列级联的检测头，每个检测头都负责边界框回归和分类任务。从第一个检测头开始，每个检测头都会对上一个检测头的输出边界框进行优化和精细化。这样，经过多个检测头的级联优化后，最终得到的边界框具有更高的定位精度。")]),e("li",null,[e("strong",null,"逐步提高 IoU 阈值"),i("：在级联检测头的训练过程中，每个检测头使用不同的 IoU 阈值。随着级联的进行，IoU 阈值逐渐提高，从而使得边界框回归任务更加严格。这种策略使得每个检测头都能在上一个检测头的基础上进一步优化边界框，从而提高整体检测性能。")]),e("li",null,[e("strong",null,"多阶段训练"),i("：Cascade R-CNN 采用多阶段训练策略。首先，单独训练第一个检测头；然后，在第一个检测头的基础上，依次训练后续的检测头。这种训练策略保证了每个检测头都能在上一个检测头的基础上进行优化，从而最大限度地提高检测性能。")])])],-1),f=e("h2",{id:"one-stage",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#one-stage","aria-hidden":"true"},"#"),i(" One-stage")],-1),N=e("p",null,"SSD算法",-1),E=e("p",null,[e("img",{src:"https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/image-20230425092956475.png",alt:"image-20230425092956475"})],-1),C=e("p",null,[e("strong",null,"多尺度特征图"),i("：SSD 使用多个尺度的特征图来进行检测。较低层次的特征图负责检测较小的物体，而较高层次的特征图负责检测较大的物体。")],-1),F=e("p",null,"RetinaNet",-1),A=e("p",null,[e("img",{src:"https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/image-20230425101901931.png",alt:"image-20230425101901931"})],-1),w=e("h2",{id:"anchor-free",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#anchor-free","aria-hidden":"true"},"#"),i(" Anchor-free")],-1),P=e("p",null,"基于Anchor的目标检测算法主要有以下四大缺点：",-1),x=e("ul",null,[e("li",null,"Anchor的大小，数量，长宽比对于检测性能的影响很大(通过改变这些超参数Retinanet在COCO benchmark上面提升了4%的AP)，因此Anchor based的检测性能对于anchor的大小、数量和长宽比都非常敏感。"),e("li",null,"这些固定的Anchor极大地损害了检测器的普适性，导致对于不同任务，其Anchor都必须重新设置大小和长宽比。"),e("li",null,"为了去匹配真实框，需要生成大量的Anchor，但是大部分的Anchor在训练时标记为负样本，所以就造成了样本极度不均衡问题(没有充分利用fore-ground)。"),e("li",null,"在训练中，网络需要计算所有Anchor与真实框的IOU，这样就会消耗大量内存和时间。")],-1),S=e("hr",null,null,-1),k=e("p",null,"CornerNet",-1),R=e("p",null,[e("img",{src:"https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/image-20230425132730680.png",alt:"image-20230425132730680"})],-1),y=e("li",null,[e("strong",null,"骨干网络"),i("：CornerNet 使用 Hourglass 网络作为骨干网络。Hourglass 网络包括一个编码器-解码器结构，逐渐降低分辨率以捕获更大的上下文信息，然后再逐渐提高分辨率以重建目标检测所需的细节。")],-1),B=e("li",null,[e("strong",null,"检测头"),i("：CornerNet 使用两个独立的检测头来预测左上角和右下角关键点的heatmap、Embeddings(左上角和右下角关键点的Embeddings相似，则为同一组)、Offsets（调整框的位置）")],-1),v=l('<li><p>CenterNet</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/image-20230425134659757.png" alt="image-20230425134659757"></p><p>图片经过Backbone之后会经过两个分支，上面的分支跟CornerNet一样，会输出热力图、Embeddings、Offsets，得到一对corners；下面的分支也会输出热力图、offsets，得到一个中心点。最终通过角点和中心点，得到预测框。</p><p>具体过程为：根据中心点热力图，选出置信度高的top-k个中心点，根据offset值调整中心点的位置，然后为角点部分预测框计算一个中心的矩形区域，如果中心点落入中心区域，则保留该预测框，并将左上角点、右下角点以及中心点这3个位置的置信度均值作为预测框最终的置信度。</p><p>提出了<a href="../%E7%A7%91%E7%A0%94/tricks/Center_pooling">Center Pooling</a>以及[Cascade corner pooling](../科研/tricks/corner_pooling.md#Cascade corner pooling)（改进了corner pooling）</p></li><li><p>FSAF</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/image-20230425151859375.png" alt="image-20230425151859375"></p><p>使用一个骨干网络（例如 ResNet）来提取输入图像的特征。</p><p>在RetinaNet的基础上，FSAF加入了一个与anchor-base branch并行的anchor-free branch</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/image-20230425153531373.png" alt="image-20230425153531373"></p><p>使用 Anchor-Free 的检测头来预测目标的类别和位置。检测头直接在特征图上预测目标。</p><p>FSAF 引入了一个特征选择模块，用于为每个目标自动选择最合适的特征图（计算目标在每个特征图上的损失，损失包含focal loss和IoU loss，选择损失最小的进行优化）。</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/image-20230425153513690.png" alt="image-20230425153513690"></p></li><li><p>FCOS</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/image-20230425211729271.png" alt="image-20230425211729271"></p></li>',3),I=e("h2",{id:"数据集",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#数据集","aria-hidden":"true"},"#"),i(" 数据集")],-1),L={href:"https://arleyzhang.github.io/articles/1dc20586/",target:"_blank",rel:"noopener noreferrer"},O={href:"https://cloud.tencent.com/developer/article/1747599",target:"_blank",rel:"noopener noreferrer"},j={href:"https://blog.csdn.net/qq_41185868/article/details/82939959",target:"_blank",rel:"noopener noreferrer"},D={href:"https://bbs.easyaiforum.cn/thread-20-1-1.html",target:"_blank",rel:"noopener noreferrer"},$=l('<h2 id="评价指标" tabindex="-1"><a class="header-anchor" href="#评价指标" aria-hidden="true">#</a> 评价指标</h2><p>$AP_{50}$：</p><ol><li>对于给定的 IoU 阈值（0.5），计算模型在每个类别上的 Precision 和 Recall。</li><li>改变置信度阈值，计算不同置信度下的 Precision 和 Recall。</li><li>对每个类别的 Precision 进行插值以获得平滑的 Precision-Recall 曲线。</li><li>计算 Precision-Recall 曲线下的面积，即 AP50。</li></ol><p>mAP：考虑了所有类别 AP 值的平均值。对于每个类别，计算 AP 时会考虑多个 IoU 阈值（例如，在 COCO 数据集中，通常使用 0.5 到 0.95 之间的值，以 0.05 为间隔）。</p><h1 id="小目标检测" tabindex="-1"><a class="header-anchor" href="#小目标检测" aria-hidden="true">#</a> 小目标检测</h1><blockquote><p>Cheng, Gong, et al. &quot;Towards large-scale small object detection: Survey and benchmarks.&quot; <em>arXiv preprint arXiv:2207.14096</em> (2022).</p></blockquote><h2 id="难点" tabindex="-1"><a class="header-anchor" href="#难点" aria-hidden="true">#</a> 难点</h2><p>小目标检测(SOD)是目标检测最主要的难点，在检测小目标和正常尺寸目标方面存在巨大的性能差距。以DyHead为例，DyHead在COCO测试集上小目标的平均精度（mAP）度量仅为28.3%，显著落后于中型和大型目标（分别为50.3%和57.5%）。作者认为原因有两点：</p><ol><li><p>小目标本身的像素是有限且扭曲的，而主流的特征提取器，都是下采样特征图来学习高维的特征，因此对于小目标难以学习到好的表征。</p></li><li><p>容易被背景或其他目标干扰。</p></li><li><p>对边界框的容忍性很低</p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/21/image-20230421083205168.png" alt="image-20230421083205168" style="zoom:50%;"></li><li><p>缺乏针对小目标检测的大型数据集</p></li></ol><h2 id="方法" tabindex="-1"><a class="header-anchor" href="#方法" aria-hidden="true">#</a> 方法</h2><p>对于小目标检测面对的挑战，通常在上述方法中引入一些有针对性的设计，下面会做出介绍 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/23/image-20230423134931758.png" alt="image-20230423134931758"></p><h3 id="针对样本" tabindex="-1"><a class="header-anchor" href="#针对样本" aria-hidden="true">#</a> 针对样本</h3><ul><li>小目标在数据集中数量少</li><li>现有的重叠匹配的策略过于严格（难以采样到足够的positive anchors）</li></ul><p>对于上面两种挑战，产生了两种方向：通过数据增强增加小目标的数量；设计最优分配策略</p><h4 id="数据增强" tabindex="-1"><a class="header-anchor" href="#数据增强" aria-hidden="true">#</a> 数据增强</h4><ol><li>Kisantal等人[55]：通过复制小物体并将其粘贴到原始图像的不同位置（随机变换），从而实现数据增强。</li><li>RRNet [56]：引入了名为AdaResampling的自适应数据增强策略。该策略遵循与[55]相同的思路，主要区别在于使用先验分割图来引导粘贴位置的采样过程，并对粘贴的物体进行缩放变换以减少尺度差异。</li><li>Zhang等人[57]和Wang等人[58]：都采用了基于分割和调整尺寸的方法来生成更多的小物体训练样本。</li><li>DS-GAN [59]：结合了物体分割、图像修复和图像融合技术，设计了一个新颖的数据增强流程，用于生成高质量的小物体合成数据。</li></ol><p>这些方法都是通过对小物体进行复制、粘贴、调整尺寸等操作，从而生成更多丰富多样的训练样本，有助于提高模型在处理小物体检测和识别任务时的性能。</p><h4 id="最优分配" tabindex="-1"><a class="header-anchor" href="#最优分配" aria-hidden="true">#</a> 最优分配</h4><ol><li>S3FD [60]：通过设计的尺度补偿锚点匹配策略，S3FD增加了与微小人脸匹配的锚点数量，从而提高了召回率。</li><li>朱等人[61]：提出了预期最大重叠（EMO）得分，该得分在计算重叠时考虑了锚点步幅，从而为小人脸找到更好的锚点设置。</li><li>徐等人[62]：使用提出的DotD（定义为两个边界框中心点之间的归一化欧几里得距离）来替换通常使用的IoU（交并比）。</li><li>RFLA [63]：在标签分配中，RFLA通过衡量每个特征点的高斯接受域与ground truth之间的相似性，从而提高主流检测器在微小物体上的性能。</li></ol><p>这些方法在计算锚点和ground truth之间的相似性时，这些方法引入了新的度量，如EMO得分、DotD和高斯接受域，以解决基于重叠匹配策略和先验设计带来的问题。</p><hr><p>这类方法都是为了得到足够正样本。基于增强的方法：这类方法通常在性能改进和迁移能力方面表现不一致。这意味着它们可能在某些情况下取得很好的效果，但在其他情况下表现不佳。优化标签分配方案：尽管这些方案试图提高小物体的性能，但它们容易引入低质量的样本，并且在处理尺寸极限较小的物体时仍然面临挑战。</p><h3 id="尺度感知" tabindex="-1"><a class="header-anchor" href="#尺度感知" aria-hidden="true">#</a> 尺度感知</h3><p>这方面工作主要沿着两条路径发展：</p><ol><li>构建针对特定尺度的检测器：通过设计多分支架构或量身定制的训练方案，构建适用于特定尺度的检测器。</li><li>融合层次特征：这类方法试图将不同层次的特征融合，以便为小物体生成强大的表示。</li></ol><h4 id="特定尺度检测器" tabindex="-1"><a class="header-anchor" href="#特定尺度检测器" aria-hidden="true">#</a> 特定尺度检测器</h4><ol><li>针对特定尺度物体的检测器：通过==对特定尺度的物体进行专门处理来改善性能==。这些方法包括Yang等人[69]提出的尺度依赖池化（SDP）、MS-CNN[70]、DSFD[71]、YOLOv3[45]、特征金字塔网络（FPN）[2]等。这些方法通过为不同尺度的物体分配不同的特征层次或网络架构来实现专门的处理。</li><li>融合多尺度检测器：这类方法通过==组合不同尺度的检测器来实现多尺度检测==。例如，Li等人[75]构建了用于检测小行人的小尺寸子网络，SSH[76]组合了不同尺度范围的人脸检测器，TridentNet[77]采用了多分支架构，QueryDet[78]设计了级联查询策略等。</li><li>定制的数据准备策略：==在训练过程中强制检测器关注特定尺度的实例==。例如，Singh等人[79]提出了尺度归一化图像金字塔（SNIP），Sniper[80]建议从多尺度图像金字塔中采样，Najibi等人[81]提出了一种粗到细的检测小物体的流程，Chen等人[82]设计了一种反馈驱动的训练范式，Yu等人[7]引入了一种基于统计的匹配策略等。</li></ol><h4 id="层次特征融合" tabindex="-1"><a class="header-anchor" href="#层次特征融合" aria-hidden="true">#</a> 层次特征融合</h4><p>低层特征具有更多的定位信息和细节表达，而高层特征包含丰富的语义信息。许多方法==通过融合不同深度的特征来获得更好的小物体特征表示==。 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/24/640.jpeg" alt="图片"> 其中FPN自从被提出来，先后迭代了不少版本。大致迭代路径如下图： <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/640-20230425220139674.jpeg" alt="图片"></p><ol><li><p>无融合</p><p>最典型的就是SSD算法，直接利用不同stage的特征图分别负责不同scale大小物体的检测</p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/image-20230425092956475.png" alt="image-20230425092956475"></p></li><li><p>自上而下单向融合</p><p>这仍然是当前物体检测模型的主流融合模式。如常见的Faster RCNN、Mask RCNN、Yolov3、RetinaNet、Cascade RCNN等</p></li><li><p>简单双向融合</p><p>FPN自从提出来以后，均是只有从上向下的融合，PANet是第一个提出从下向上二次融合的模型，并且PANet就是在Faster/Mask/Cascade RCNN中的FPN的基础上，简单增了从下而上的融合路径。</p></li></ol><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/640-20230425220639406.jpeg" alt="图片"></p><ol><li><p>复杂的双向融合</p><p>PANet的提出证明了双向融合的有效性，而PANet的双向融合较为简单，因此不少文章在FPN的方向上更进一步，尝试了更复杂的双向融合，如ASFF、NAS-FPN和BiFPN。</p><blockquote><p>==Liu, Songtao, Di Huang, and Yunhong Wang. &quot;Learning spatial fusion for single-shot object detection.&quot; <em>arXiv preprint arXiv:1911.09516</em> (2019).==</p></blockquote><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/640-20230425222010892.jpeg" alt="图片"></p><ul><li>作者在YOLOV3的FPN的基础上，研究了每一个level再次融合三个level特征的效果</li><li>add基础上多了一个可学习系数（1×1卷积），该参数是自动学习的，可以实现自适应融合效果，类似于全连接参数。</li></ul><blockquote><p>Tan, Mingxing, Ruoming Pang, and Quoc V. Le. &quot;Efficientdet: Scalable and efficient object detection.&quot; <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2020.</p></blockquote><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/{year}/{month}/{day}/{filename}{.suffix}image-20230426100749249.png" alt="image-20230426100749249"> $$ P_6^{td}=Conv\\left(\\dfrac{w_1\\cdot P_6^{in}+w_2\\cdot Resize(P_7^{in})}{w_1+w_2+\\epsilon}\\right) $$ $$ P_6^{out}=Conv\\left(\\dfrac{w&#39;_1\\cdot P&#39;_6+w&#39;_2\\cdot P&#39;_6+w&#39;_3\\cdot Resize(P&#39;_5)}{w&#39;_1+w&#39;_2+w&#39;_3+\\epsilon}\\right) $$</p><ul><li>加权特征融合：BiFPN使用可学习的权重对不同尺度的特征图进行加权融合。这使得网络能够自动学习不同尺度特征之间的最佳权重，从而在融合过程中更好地利用每个尺度的信息。</li><li>深度可分离卷积：为了减少计算成本和参数数量，BiFPN使用深度可分离卷积（depthwise separable convolution）替换传统的卷积操作。这种卷积方法在保持良好性能的同时显著降低了计算复杂度和内存需求。</li><li>重复多次的BiFPN结构：BiFPN通常在网络中重复多次，以便在整个金字塔中进行更多次的特征融合。这有助于进一步提高不同尺度特征之间的互操作性，从而提高整体性能。</li></ul><blockquote><p>Qiao, Siyuan, Liang-Chieh Chen, and Alan Yuille. &quot;Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution.&quot; <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2021.</p></blockquote><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/{year}/{month}/{day}/{filename}{.suffix}image-20230426104352556.png" alt="image-20230426104352556"></p><ul><li>递归结构：Recursive-FPN的核心创新之处在于它采用了递归结构，使特征金字塔的特征融合过程可以重复进行多次。在每次迭代中，特征金字塔的特征图都会进行自上而下和自下而上的融合。通过多次迭代，Recursive-FPN能够在不同尺度的特征图之间实现更强大的特征融合。</li><li>ASPP是通道上的叠加 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/{year}/{month}/{day}/{filename}{.suffix}image-20230426105231944.png" alt="image-20230426105231944"></li><li>Fusion是使用$f^{t+1}_i$计算attention map，再将$f^{t+1}_i$和$f^{t}_i$相加 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/{year}/{month}/{day}/{filename}{.suffix}{filename}{.suffix}image-20230426105328556.png" alt="image-20230426105328556"></li></ul></li></ol><h3 id="注意力机制" tabindex="-1"><a class="header-anchor" href="#注意力机制" aria-hidden="true">#</a> 注意力机制</h3><p>视觉注意力机制通过为特征图的不同部分分配不同的权重，强调有价值的区域，同时抑制不必要的区域。这种机制可以用于突出图像中容易被背景和噪声模式所支配的小物体。</p><ol><li><h2 id="scrdet-102-设计了一个面向小物体的检测器-通过有监督地训练像素注意力和通道注意力-突出小物体区域-消除噪声干扰。anchor-based方法特征图的大小决定了anchor的数量。特征图过大-语义信息少-同时anchor数量过多-会导致iou提高-效率较低-特征图过小-anchor数量少-不能很好的捕获小目标。因此作者将c3和c4调整为预设anchors步长s-实验发现s-6时-模型表现最好-第二个分支添加了一个inception结构-包含各种尺寸的卷积核-以捕获目标形状的多样性。" tabindex="-1"><a class="header-anchor" href="#scrdet-102-设计了一个面向小物体的检测器-通过有监督地训练像素注意力和通道注意力-突出小物体区域-消除噪声干扰。anchor-based方法特征图的大小决定了anchor的数量。特征图过大-语义信息少-同时anchor数量过多-会导致iou提高-效率较低-特征图过小-anchor数量少-不能很好的捕获小目标。因此作者将c3和c4调整为预设anchors步长s-实验发现s-6时-模型表现最好-第二个分支添加了一个inception结构-包含各种尺寸的卷积核-以捕获目标形状的多样性。" aria-hidden="true">#</a> SCRDet [102]：设计了一个面向小物体的检测器，通过有监督地训练像素注意力和通道注意力，突出小物体区域，消除噪声干扰。 anchor-based方法特征图的大小决定了anchor的数量。特征图过大，语义信息少，同时anchor数量过多，会导致IoU提高，效率较低；特征图过小，anchor数量少，不能很好的捕获小目标。因此作者将C3和C4调整为预设anchors步长S（实验发现S=6时，模型表现最好），第二个分支添加了一个Inception结构，包含各种尺寸的卷积核，以捕获目标形状的多样性。 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/{year}/{month}/{day}/{filename}{.suffix}image-20230426161722275.png" alt="image-20230426161722275" style="zoom:50%;"></h2><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/{year}/{month}/{day}/{filename}{.suffix}image-20230426192534312.png" alt="image-20230426192534312"> F3特征图经过两个分支：上分支关注空间像素的重要性，计算得到的显著图和二值图计算交叉熵损失；下分支则计算通道注意力。两种注意力机制相互补充</li><li>FBR-Net [103]：在锚点无关的检测器 FCOS [4] 的基础上引入基于级别的注意力机制，平衡不同金字塔层次上的特征，并在复杂情况下提高小物体的学习效果。</li><li>KB-RANN [104]：利用长期和短期注意力神经网络关注图像特征的特定部分，增强小目标的检测。</li><li>Lu et al. [105]：设计了一个双路径模块，突出小物体的关键特征，抑制非物体信息。</li><li>MSCCA [106]：通过使用增强通道注意力（ECA）模块替换复杂的卷积组件，构建了一个具有平衡通道特征且参数较少的轻量级检测器。</li><li>Li et al. [107]：设计了一个跨层注意力模块，以获得小目标的更强响应。 注意力系列方法具有灵活的嵌入设计，可以应用于几乎所有 SOD 架构。然而，性能的提升往往需要付出较大的计算开销，因为注意力模型中的相关操作通常较为复杂。</li></ol><h3 id="特征模仿" tabindex="-1"><a class="header-anchor" href="#特征模仿" aria-hidden="true">#</a> 特征模仿</h3><p>由于小实例的信息量较少，导致的低质量表示问题。对于尺寸极小的物体，这个问题可能会变得更严重。同时，较大的实例通常具有更清晰的视觉结构和更好的区分度。因此，一个直接解决这一问题的方法是通过模仿较大物体的区域特征来丰富小物体的区域特征。</p><ol><li>通过相似性学习进行特征模仿：这类方法旨在学习小物体特征与大物体特征之间的相似性，从而提高小物体的表示质量。</li><li>基于超分辨率的框架：这类方法通过将小物体的低分辨率特征映射到高分辨率空间，以提高其表示质量。</li></ol><h4 id="相似性学习" tabindex="-1"><a class="header-anchor" href="#相似性学习" aria-hidden="true">#</a> 相似性学习</h4><p>方法的核心原理是==在通用目标检测器的训练过程中施加额外的相似性约束，从而缩小小物体与大物体之间的表示差距==。</p><ol><li>Wu等人[109]提出了一种自我模仿学习（Self-Mimic Learning）方法。在这个方法中，强制使小尺度行人的表示靠近大尺度行人的局部平均 RoI 特征。通过这种方式，提高小尺度行人的表示质量。</li><li>受人类视觉理解机制中记忆过程的启发，Kim等人[110]设计了一种大尺度嵌入学习方法，该方法包含了一个大尺度行人回忆记忆（Large-scale Pedestrian Recalling Memory，简称 LPR Memory）。整个架构在回忆损失（recalling loss）的指导下进行优化，其目的是引导小尺度和大尺度行人特征变得相似。</li></ol><h4 id="超分辨率" tabindex="-1"><a class="header-anchor" href="#超分辨率" aria-hidden="true">#</a> 超分辨率</h4><p>超分辨率方法的目标是==恢复小物体的失真结构==，而不仅仅是简单地放大其模糊的外观。</p><ol><li>利用反卷积和子像素卷积[111]，Zhou等人[84]和Deng等人[112]获得了专门用于小物体检测的高分辨率特征。</li><li>基于自监督学习范式，Pan等人[113]提出了一个引导特征上采样模块，用于学习具有详细信息的放大特征表示。</li><li>生成对抗网络（GAN）[114]可以通过生成器和判别器之间的两人极小极大博弈生成视觉上真实的数据，这使得研究人员将这一强大范式用于生成小物体的高质量表示。Rabbi等人[115]和Bashir等人[116]都使用GAN对低分辨率遥感图像进行超分辨率重建。</li><li>MTGAN[117]通过生成器网络对RoI的区域进行超分辨率重建，以减少特征提取阶段的计算成本。</li><li>有研究者将超分辨率方法扩展至人脸检测任务和小候选区域，以提高性能。</li><li>超分辨率方法可能忽略了对网络预测起关键作用的上下文信息。为解决这个问题，有研究者提出了PerceptualGAN[122]，以挖掘并利用小尺度和大尺度物体之间的内在关联。此外，Noh等人[120]在超分辨率过程中引入了直接监督。</li></ol><hr><p>通过向现有检测器添加额外的相似性损失或超分辨率结构，特征模仿方法使模型能够挖掘小尺度物体与大尺度物体之间的内在关联，从而增强小物体的语义表示。然而，相似性学习方法和超分辨率方法都需要避免特征坍塌问题并保持特征多样性。此外，基于GAN的方法可能产生虚假纹理和伪影，对检测产生负面影响。更糟糕的是，超分辨率结构的存在使端到端优化变得复杂。</p><h3 id="上下文建模" tabindex="-1"><a class="header-anchor" href="#上下文建模" aria-hidden="true">#</a> 上下文建模</h3><p>上下文信息可以帮助理==解环境与物体之间以及物体与物体之间的关系，从而有助于识别物体和场景==。有时候信息丰富的上下文比物体本身提供更多的决策支持，尤其是在识别质量较差的物体时。因此，有一些方法利用上下文线索来提高小物体的检测性能。</p><ol><li>Chen等人[28]：利用包含建议补丁的上下文区域的表示来进行后续识别。</li><li>Hu等人[128]：研究如何有效地编码物体范围之外的区域并以尺度不变的方式建模局部上下文信息，以便检测微小的人脸。</li><li>PyramidBox [124]：充分利用上下文线索找到难以从背景中区分的小且模糊的面部。 尽管利用上下文信息对小物体检测有所帮助，目前的上下文建模机制在确定上下文区域时仍存在问题，因为它们以启发式和经验的方式确定，不能保证构建的表示对于检测来说足够具有解释性。</li></ol><h1 id="论文" tabindex="-1"><a class="header-anchor" href="#论文" aria-hidden="true">#</a> 论文</h1><blockquote><p>Wang, Wenhai, et al. &quot;Internimage: Exploring large-scale vision foundation models with deformable convolutions.&quot; <em>arXiv preprint arXiv:2211.05778</em> (2022).</p></blockquote><ul><li>提出一种新的CNN大模型——InternImage，首个参数达1B、训练数据达400M、取得与ViT相当甚至更优性能的CNN模型。证明对于大尺度模型研究，CNN同样是一个值得探索的方向。</li><li>将长程依赖、自适应空域聚合引入到模型，将CNN模型的大小和尺度进行扩展；并对模块定制化、堆叠规则以及缩放策略进行了探索。</li><li>在图像分类、目标检测、语义分割以及实例分割等下游任务上验证了所提方案的有效性。其中，InternImage-B仅在ImageNet-1K训练即可取得84.9%的精度(比其他CNN至少高出1.1%)；当在大量参数(1B)、海量数据(427M)条件下，InternImage-H取得了89.2%的性能；在COCO上，InternImage-H以2.18B的参数量取得了65.4%mAP，比SwinV2-G高出2.3%，参数量少27% 。</li></ul><hr><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/27/image-20230427143602705.png" alt="image-20230427143602705" style="zoom:50%;">',54),q=e("img",{src:"https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/27/image-20230427151130882.png",alt:"image-20230427151130882",style:{zoom:"50%"}},null,-1),V=e("hr",null,null,-1),Y=e("p",null,"在ImageNet1k上的结果，超越了同等大小下的VAN、RepLKNet、ConvNext等先进的大核注意力CNN模型，同时也超过了Swin、DeiT3、CoAtNet等ViT模型。 InternImage-H在目标检测COCO数据集上达到了65.5mAP的性能，超越了SwinV2-G大模型的性能（参数量还少了27%）。是唯一达到65.0mAP的模型",-1),M=e("blockquote",null,[e("p",null,[i('Fang, Yuxin, et al. "Eva: Exploring the limits of masked visual representation learning at scale." '),e("em",null,"arXiv preprint arXiv:2211.07636"),i(" (2022).")])],-1);function G(z,H){const a=r("RouterLink"),o=r("ExternalLinkIcon");return c(),h("div",null,[p,e("p",null,[i("推理过程就是预测出所有的anchor，然后根据阈值对anchor进行过滤，再将anchor映射回原图，进行"),t(a,{to:"/%E7%A7%91%E7%A0%94/tricks/NMS%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6.html"},{default:n(()=>[i("NMS非极大值抑制")]),_:1}),i("，最后的到所有的预测框。")]),m,e("ul",null,[e("li",null,[e("p",null,[i("RCNN 首先使用"),t(a,{to:"/%E7%A7%91%E7%A0%94/%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/%E9%80%89%E6%8B%A9%E6%80%A7%E6%90%9C%E7%B4%A2.html"},{default:n(()=>[i("选择性搜索")]),_:1}),i("（Selective Search）算法生成候选区域，然后将这些区域输入到 CNN 中提取特征，最后使用 SVM 进行分类和线性回归进行边界框回归。 "),d,i(" ==不足==：处理速度较慢，因为每个候选区域都需要独立地输入到 CNN 中进行特征提取。")])]),g,e("li",null,[e("p",null,[i("Faster RCNN Faster RCNN 的主要创新是引入了 Region Proposal Network（RPN），用于生成候选区域。RPN 是一个全卷积神经网络，可以直接在特征图上生成候选区域，避免了使用选择性搜索等传统方法的计算开销。这使得 Faster RCNN 在保持较高准确率的同时，实现了更快的处理速度。"),t(a,{to:"/%E7%A7%91%E7%A0%94/%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/Faster_RCNN.html"},{default:n(()=>[i("Faster RCNN训练方法")]),_:1}),_])]),b]),f,e("ul",null,[e("li",null,[e("p",null,[t(a,{to:"/%E7%BB%84%E4%BC%9A/2023.04.27%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A6%82%E8%BF%B0.html#YOLO%E7%B3%BB%E5%88%97"},{default:n(()=>[i("Yolo系列算法")]),_:1})])]),e("li",null,[N,E,C,e("p",null,[i("在边框预测回归问题上使用到的是"),t(a,{to:"/%E7%A7%91%E7%A0%94/tricks/smooth_L1_loss.html"},{default:n(()=>[i("smooth L1 loss")]),_:1})])]),e("li",null,[F,A,e("p",null,[i("作者深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 "),t(a,{to:"/%E7%A7%91%E7%A0%94/tricks/Focal_loss.html"},{default:n(()=>[i("Focal Loss")]),_:1}),i("焦点损失函数")])])]),w,P,x,S,e("ul",null,[e("li",null,[k,R,e("ol",null,[y,B,e("li",null,[i("提出了一种适用于cornerNet网络的"),t(a,{to:"/%E7%A7%91%E7%A0%94/tricks/corner_pooling.html"},{default:n(()=>[i("corner pooling")]),_:1})])])]),v]),I,e("ul",null,[e("li",null,[e("a",L,[i("PASCAL VOC"),t(o)]),i("：train/val ：11540 张图片，包含 27450 个已被标注的 ROI annotated objects")]),e("li",null,[e("a",O,[i("ILSVRC"),t(o)]),i("：IMAGENET Large Scale Visual Recognition Challenge，超过百万的图片有明确的类别标注和图像中物体位置的标注")]),e("li",null,[e("a",j,[i("MS-COCO"),t(o)]),i("：包含20万个图像；80个类别中有超过50万个目标标注")]),e("li",null,[e("a",D,[i("Open Images(QID)"),t(o)]),i("：约900万张的链接图像，横跨了大约6000个类别")])]),$,i(" 论文基于DCN V2做出了三点关键改进，提出了DCN V3。 在提出DCN V3算子前，作者先回顾了一下传统卷积和多头自注意力机制这两种算子的区别，主要包括如下两点： - **长距离建模能力**。普通的卷积聚合邻域内的信息，显然不具备全局建模的能力，尽管传统的CNN能够通过堆叠多个3$\\times$3的卷积或者使用大卷积核来增加模型的感受野，但是仍然不能像ViT那样进行全局的交互。 - **自适应空间聚合能力**。多头自注意力机制在聚合不同的tokens的时候，权重都是根据输入query的不同而动态变化的；而传统的CNN不管输入是什么，卷积核的参数都是静态不变的。 ![img](https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/27/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p1c3Rzb2xvdw==,size_16,color_FFFFFF,t_70.png) $$ \\mathbf{y}\\left(p_0\\right)=\\sum_{k=1}^{K}\\mathbf{w}_k\\mathbf{m}_k\\mathbf{x}\\left(p_0+p_k+\\Delta p_k\\right) $$ 调制标量$\\mathbf{m}_k$和偏移量$\\Delta p_k$是可学习的 尽管DCN V2算子已经缩小了普通卷积算子和MHSA之间的差距，对于大规模的视觉基础模型来说，DCN V2算子仍然不是最优的选择，于是作者从三个方面对DCN V2进行改进，得到了IntenImage的主要算子——DCN V3。 $$ \\textbf{y}\\left(p_0\\right)=\\sum_{g=1}^{G}\\sum_{k=1}^{K}\\textbf{w}_g\\textbf{m}_{gk}\\textbf{x}_g\\left(p_0+p_k+\\bigtriangleup p_{gk}\\right) $$ 1. **共享卷积权重**：一组中的采样点使用相同的权重，使用$\\mathbf{m}_{gk}$来进行加权 2. **引入多组机制**：借鉴多头自注意力机制中的“多头”思想，将DCN V3进行分组，相当于把原来的操作重复多次，增强了DCN V3算子的表达能力 3. **归一化调制标量**：对K个调制标量$\\mathbf{m}_k$进行softmax归一化，使得整个训练过程更为稳定。 "),q,i(" 学习了transformer中的结构，以及Layer Normalization (LN), 前馈神经网络 FFN, GELU 。并且根据堆叠规则以及缩放策略对模型进行缩放 ![image-20230427151349817](https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/27/image-20230427151349817.png) "),V,Y,M])}const U=s(u,[["render",G],["__file","2023.04.27目标检测概述.html.vue"]]);export{U as default};
