import{_ as e,p as a,q as t,a1 as i}from"./framework-e03faf0e.js";const n={},s=i('<h1 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要" aria-hidden="true">#</a> 摘要</h1><p>面部表情识别是一个具有挑战性的问题，因为面部表情成分总是与其他无关因素纠缠在一起，如身份和头部姿势。 作者认为身份和姿态信息干扰了面部表达识别，因此提出了将身份和姿态信息解耦后再进行面部表情识别的 IPD-FER 模型，这样可以学习更具有区分度的表情特征 身份、姿态和表情这三个部分分别被三个编码器进行编码：</p><ol><li>对于身份编码器，利用了一个预先训练好的人脸识别模型，并在训练过程中固定不变，这减轻了之前工作中对特定表情训练数据的限制，并使得解耦在自然数据集上成为可能。</li><li>姿态和表情编码器则在训练时被相应的优化</li></ol><p>结合输入的身份和姿态特征，解码器可以输出一个中性的面部特征，如果再加上表情特征，则解码器可以输出一个带有表情的面部特征。再通过比较这个人的中性面部特征和表情特征，身份特征和姿态信息就可以进一步的从表情特征中解耦出来 实验结果也表明无论在实验室数据上还是更自然的数据集上，模型表现都达到了 SOTA 水平</p><h1 id="介绍" tabindex="-1"><a class="header-anchor" href="#介绍" aria-hidden="true">#</a> 介绍</h1><p>由于身份信息与表情特征进行纠缠，导致同一个人表情的差异比不同人的表情差异更小，按理说两个的差异应该相似。因此作者认为学习到的表达表示仍然是身份主导信息，没有足够的训练数据，这使得模型不能很好地泛化看不见的身份。 <strong>身份解耦</strong> 一些方法比较表情面部和中性面部对的差别，来移除身份信息，但是这种解耦算法非常的依赖特定的数据集，这个数据集必须得拥有大量的表情面部、中性面部对，不利于在现实数据中普及 TDGAN 和 TER-GAN 使用编码器的两个分支分别提取身份和表达特征。将两个个体 A 和 B 的两张人脸图像输入到网络中，然后将两个编码器的特征组合在一起，构建一个与 A 保持相同身份但与 B 保持相同表情的新图像。然而，它们在野外数据上的表现也不太好 <strong>姿态解耦</strong> 许多方法已经提出来实现对姿态进行解耦的 FER，但是，它们依赖于多视角数据集，并且无法推广到自然环境（in-the-wild）数据上。 也有方法提出了一个通过对抗学习训练的身份和姿态解耦的模型, 但这需要身份标签，而这些标签在现有的自然环境数据库中并不可用。</p><h1 id="模型" tabindex="-1"><a class="header-anchor" href="#模型" aria-hidden="true">#</a> 模型</h1><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/r181dN.jpg" alt="r181dN"> 身份、姿态和表情这三个部分分别被三个编码器进行编码, 再根据这三个特征利用解码器对中性面部和表情面部进行重建, 这样它们两个之间的差别只有表情成分, 使得 FER 可以获得更具区分度的表情特征</p><h2 id="生成器" tabindex="-1"><a class="header-anchor" href="#生成器" aria-hidden="true">#</a> 生成器</h2><p><strong>身份编码器</strong> 作者会使用一个预训练好的身份识别器来提取身份特征, 且在训练过程中被固定. 因为通常好的身份识别器都是在百万级别的数据集上训练, 因此这个特征不会与姿态和表情有关, 可以作为身份参考, 识别出来的特征是中性和正面的表情. 这个方法减轻了以往身份解耦算法中对身份标签或中性表达图像对的要求，使解耦在自然数据库上可行。 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/wAmGCz.jpg" alt="wAmGCz"><strong>姿态和表情编码器</strong> 姿态和表情编码器则利用姿态标签和表情表情被训练.</p><hr><p>身份特征 $f_{id}$ 和姿态特征 $f_{pose}$ 结合得到身份姿态特征 $f_{i-p}$,再通过解码器, 就得到了中性面部 $x^f_{i-p}$.类似的, 再加入表情特征就得到了重建后的输入 $x^f_{i-p-e}$</p><hr><p><strong>损失函数</strong> 为了生成更准确的图片对, 也就是中性面部和表情面部, 因此需要利用重构损失来保证合成图像在外观上与输入图像相似. $$ \\mathcal{L}<em>{recon}=\\mathbb{E}</em>{(x,y_e)}|x_{i-p-e}^f-x|<em>1+\\mathbb{E}</em>{(x,y_e)}1[y_e=c]|x_{i-p}^f-x|<em>1 $$ 其中 c 表示表情标签为中性, 当表情为中性时, 就加上后面一项损失, 希望重构的中性面部也能与输入的表情面部相似 令 $N$ 表示为预训练的身份编码器, 目标是输入的表情面部和重构的中性面部和表情面部, 它们的身份特征应该是相同的 $$ \\mathcal{L}</em>{id}=\\mathbb{E}<em>x|N(x</em>{i-p-e}^f)-N(x)|<em>1+\\mathbb{E}<em>x|N(x</em>{i-p}^f)-N(x)|<em>1 $$ 为了让身份特征和表情特征进一步分离, 作者提出了相似损失, 来使得它们相互正交, 这个相似度使用了余弦距离来衡量 $$ \\mathcal{L}</em>{cos}=\\mathbb{E}<em>x\\frac{|f</em>{id}\\cdot f</em>{exp}|}{||f_{id}||<em>2\\cdot||f</em>{exp}||<em>2} $$ 同理, 为了让姿态特征和表情特征分离, 提出了对抗损失, 作者认为 $f</em>{exp}$ 应该误导 $C_p$ 预测的表情类别分布, 使其趋向于均匀分布 $$ \\mathcal{L}<em>{confusion}=-\\mathbb{E}<em>x\\sum</em>{k=1}^{K_P}\\frac{1}{K_P}\\log\\frac{e^{C_p(E</em>{exp}(x))<em>k}}{\\sum</em>{j=1}^{K_P}e^{C_p(E_{exp}(x))_j}} $$ $K_P$ 表示姿态的类别数</p><h2 id="分类器" tabindex="-1"><a class="header-anchor" href="#分类器" aria-hidden="true">#</a> 分类器</h2><p>只有身份编码器是使用预训练的, 因此姿态编码器和表情编码器需要参与有监督的训练. 本文使用 MLPs 来作为分类器 $C_{exp}$ 和 $C_p$,采用交叉熵损失 $$ \\mathcal{L}<em>{exp}=-\\mathbb{E}</em>{(x,y_e)}\\sum\\limits_{k=1}^K\\mathbb{I}[y_e=k]\\log\\frac{e^{C_{exp}(E_{exp}(x))<em>k}}{\\sum</em>{j=1}^K e^{C_{exp}(E_{exp}(x))<em>j}} $$ $$ \\mathcal{L}</em>{pose}=-\\mathbb{E}<em>{(x,y_p)}\\sum\\limits</em>{k=1}^{K_P}{\\mathbb{I}}[y_p=k]\\log\\frac{e^{C_p(E_{pose}(x))<em>k}}{\\sum</em>{j=1}^{K}e^{C_p(E_{pose}(x))<em>j}} $$ $K$ 表示表情的类别数, 因此定于总的分类损失为两个分类损失的和 $$ \\mathcal{L}<em>c = \\mathcal{L}</em>{exp} + \\mathcal{L}</em>{pose} $$ 由于我们的目标是识别面部表情，因此最后使用表情编码器 $E_{exp}$ 和分类器 $C_{exp}$ 进行推理。</p><h2 id="判别器" tabindex="-1"><a class="header-anchor" href="#判别器" aria-hidden="true">#</a> 判别器</h2><p>判别器 $D$ 是用来评价生成的图像是否属于期望的表情。 对于输入的真实图像 $x$，希望判别器能够很好的区分表情，使用的是交叉熵损失 $$ \\mathcal{L}^r_{exp}=-\\mathbb{E}<em>{(x,y_e)}\\sum\\limits</em>{k=1}^K\\mathbb{I}[y_e=k]\\log(D(x)) $$ $D(x)$，就表示预测的类别分布，G 应该欺骗 D，使得 D 在生成图片上也可以预测出中性或是相应的表情。为此应用了对抗损失，对于生成的中性面部，希望判别器 D 能将其表情类别识别为中性，因此损失为： $$ \\begin{aligned} \\mathcal{L}^f_{n e u} =-\\mathbb{E}<em>{(x,c)}\\sum</em>{k=1}^K\\mathbb{I}[k=c]\\log(D(G_{dec}(E_{id}(x) \\ +E_{pose}(x))) \\end{aligned} $$ $c$ 表示中性 GT 标签。相应的生成表情面部的对抗损失为： $$ \\begin{aligned} \\mathcal{L}^f_{exp}=-\\mathbb{E}<em>{(x,y_e)}\\sum\\limits</em>{k=1}^K\\mathbb{I}[y_e&amp; =k]\\log(D(G_{dec}(E_{id}(x) \\ &amp;+E_{pose}(x)+E_{exp}(x))) \\end{aligned} $$</p><h2 id="总览" tabindex="-1"><a class="header-anchor" href="#总览" aria-hidden="true">#</a> 总览</h2><p>到目前为止，表情特征与身份特征和姿态特征已经分离开来了。首先，在特征层上用不同的编码器表示。其次，表情特征体现在生成的中性面部与表情面部之间的差异。我们定义生成器 G 的生成损失为： $$ \\mathcal{L}^{&#39;}<em>G=\\lambda_1\\mathcal{L}^{f}</em>{neu}+\\lambda_2\\mathcal{L}^{f}<em>{exp}+\\lambda_3\\mathcal{L}</em>{id}+\\lambda_4\\mathcal{L}<em>{recon} $$ 损失函数的含义就是生成器 G 生成的中性面部和表情面部应该骗过判别器 D，同时生成的面部的身份特征应该是相同的，然后生成的面部应该与原输入的真实 $x$ 相同。 生成器总的 loss 需要再加上分类 loss 以及将表情特征与身份、姿态特征分离开的 $\\mathcal{L}</em>{cos}$ 和 $\\mathcal{L}<em>{confusion}$，这三个 loss 用来训练编码器和分类器 $$ \\mathcal{L}<em>G=\\mathcal{L}<em>G^{&#39;}+\\mathcal{L}<em>c+\\beta_1\\mathcal{L}</em>{cos}+\\beta_2\\mathcal{L}</em>{confusion} $$ 因此生成器 G 和判别器 D 的训练流程如下： <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/yc1K1i.jpg" alt="yc1K1i"> 首先更新编码器和分类器，然后更新判别器，最后再更新生成器，在第一步和第三步时分别加入了 $\\mathcal{L}</em>{cos}$ 和 $\\mathcal{L}</em>{confusion}$，可能是为了着重训练 $E_{exp}$。</p><h1 id="实验" tabindex="-1"><a class="header-anchor" href="#实验" aria-hidden="true">#</a> 实验</h1><h2 id="数据集" tabindex="-1"><a class="header-anchor" href="#数据集" aria-hidden="true">#</a> 数据集</h2><p>实验用到了以下五个数据集：</p><ol><li><strong>CK+（Extended Cohn-Kanade）</strong>：这是一个实验室控制的数据库，包含了 123 个受试者的 593 个视频序列。这些视频被标记为 7 个类别（愤怒，轻蔑，厌恶，恐惧，快乐，悲伤，和惊讶）。从每个序列的峰值形成的最后三帧中提取出 981 张图片。为了获得身份信息，还使用了从每个序列的第一帧收集的 327 张中性图像。然后，通过 ID 将选定的图像分为 10 折，以进行人员独立的 10 折交叉验证。</li><li><strong>RAF-DB</strong>：这是一个大规模的自然环境下的面部表情数据库，包含了来自互联网的数千个个体的约 30,000 张极具多样性的面部图像。RAF-DB 中的图像由 315 个人类编码器标记，最终的注释通过众包技术确定。每张图像都确保被约 40 个独立的标签者标记。RAF-DB 包含 12,271 个训练样本和 3,068 个测试样本，标注了七个基本的情绪类别（即，愤怒，厌恶，恐惧，快乐，中性，悲伤和惊讶）。还提供了标记为 11 个类别的复合表情，但我们的实验中并未使用。</li><li><strong>FERPlus</strong>：FERPlus 数据库是 FER 2013 的扩展。大规模且无约束的数据库 FER 2013 是通过 Google 图片搜索 API 自动创建和标记的。FER 2013 中的所有图像都已注册并调整大小为 48×48 像素。FER 2013 包含 28,709 个训练图像，3,589 个验证图像和 3,589 个带有七个表情标签的测试图像。2016 年由 Microsoft 重新标记，每张图像由 10 个人标记，构成 8 个类别（增加了轻蔑），因此具有更可靠的注释。</li><li><strong>AffectNet</strong>：大规模的 AffectNet 数据库包含超过 40 万张标记的图像。这些图像是通过三个搜索引擎和与表情相关的关键字从互联网上下载的。这是目前用于 FER 的最大数据集。与 RAF-DB 相似，我们使用标记为七个基本表情类别的图像，得到约 280,000 个训练样本。验证集中每个类别有 500 个样本，总共 3500 个验证图像。</li><li><strong>SFEW 2.0（Static Facial Expressions in the Wild）</strong>：SFEW 是通过基于面部点聚类计算的关键帧从 AFEW 数据库中选择静态帧创建的。数据是从电影片段中收集的，包含各种头部姿势、遮挡和照明条件。SFEW 2.0 是最常用的版本，它是 EmotiW 2015 的 SReco 子挑战赛的基准。它分为三个集合：训练集（958 个样本）、验证集（436 个样本）和测试集（372 个样本）。每张图像都标注为 RAF-DB 中的七种基本表情之一。由于测试集的标签不可用，我们使用验证集进行测试。</li></ol><h2 id="细节" tabindex="-1"><a class="header-anchor" href="#细节" aria-hidden="true">#</a> 细节</h2><ol><li><strong>网络架构</strong>：三个编码器和一个判别器 D 都源于在 CASIA-WebFace 数据库（人脸识别数据集）上预训练的 ResNet-18。在训练过程中，身份编码器的权重是固定的。这四个网络的输出特征的维度都是 512。$C_{exp}$ 和 $C_p$ 是全连接层。$G_{dec}$ 由五个层序列组成，包括转置卷积、实例归一化、ReLU 和 ResNet Block。它将 512 维的特征向量转换为一个 3×112×112 的图像。</li><li><strong>图像处理</strong>：对于所有数据库中的每一张图像，首先使用 MTCNN 检测面部区域，然后使用相似变换将其调整和标准化为 112×112 像素。为了获取自然环境数据库的姿态标签，使用了一个训练过的头部姿态估计器。对这些数据库，得到了五个姿态类别。将偏航角在 0°和 10°之间的图像视为正面脸。由于 CK+中的图像都是正面脸，因此去掉了姿态编码器和相应的判别器，只将面部表示编码为身份和表情的总和。</li><li><strong>训练</strong>：设置批大小为 32，使用 Adam 优化器。学习率从 0.0001 开始，每 10 个周期减少 0.1。训练总共持续 30 个周期。超参数λ1, λ2, λ3, λ4, β1, β2 分别设置为 0.001, 0.001, 1, 10, 0.5, 1。对于 RAF-DB、FERPlus、AffectNet 和 SFEW 2.0，我们使用不同的随机种子重复实验 4 次，并报告最佳和平均验证性能。</li></ol><h2 id="方法比较" tabindex="-1"><a class="header-anchor" href="#方法比较" aria-hidden="true">#</a> 方法比较</h2><p>因为许多现有的工作主要研究如何减轻身份效应，并且只在实验室控制的数据集上进行实验。所以作者在 <strong>CK+</strong> 数据集上与其他最先进的方法进行了比较 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/P5jDGH.jpg" alt="P5jDGH"></p><hr><p>在 <strong>RAF-DB</strong> 数据库上，IPD-FER（身份和姿势消融的面部表情识别）方法与其他一些考虑了身份或姿势的方法的比较 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/4CPali.jpg" alt="4CPali"> 在 <strong>FERPlus</strong>、<strong>AffectNet</strong>和<strong>SFEW 2.0</strong>这三个数据集上的比较结果 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/xO8txD.jpg" alt="xO8txD"></p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/VPpKm3.jpg" alt="VPpKm3"></p><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/iL7Wyv.jpg" alt="iL7Wyv"> 对于 SFEW 2.0 数据集，因为它包含的训练数据量有限，所以采用了预训练模型（在 RAF-DB 数据集上训练的模型）进行微调。这样做将准确率从 56.12%提高到了 58.43%。而且，即使没有预训练，IPD-FER 在 SFEW 2.0 数据库上的表现也比基线方法高出 5.55%。 作者认为是他们的将身份、姿态和表情解耦的分离策略使得表情编码器能够提取出更有区分度的特征，从而可以更好地识别表情。</p><h2 id="消融实验" tabindex="-1"><a class="header-anchor" href="#消融实验" aria-hidden="true">#</a> 消融实验</h2><p>由于在自然环境下收集的数据库（in-the-wild databases）通常不包含身份标签，所以他们只评估了 IPD-FER 对姿势影响的解耦效果。 他们从 RAF-DB 测试集中选择头部左右转动的角度大于某个值的面部图片，构造了不同的测试子集，其中每张面部图片的偏航角分别大于 10°、20°、30°和 40° 评估了基线方法 ResNet-18、ID-FER 和 IPD-FER 在这些测试子集上的识别性能，在 ID-FER 中，没有使用 $E_{pose}$，$C_p$ 和相应的损失函数，这意味着表情信息仍然与姿态交织在一起。 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/nDmQ2a.jpg" alt="nDmQ2a"> 随着头部偏角的增大，基线模型的识别准确率降低。当头部偏角大于 40°时，其准确率只有 81.49%。对于 ID-FER，由于它没有考虑姿态，其性能接近于基线模型。然而，借助姿态编码器和对抗学习，IPD-FER 在大头部偏角测试子集上保持了高识别性能。在头部偏角大于 40°的子集上，它达到了 87.66%的准确率。</p><hr><p>在 RAF-DB 数据集上使用的损失函数或模块（Lconfusion, Lcos, Lid, Lrecon, 以及鉴别器 D）对模型性能的影响 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/5Nsct4.jpg" alt="5Nsct4"></p><ol><li>如果不使用 $L_{confusion}$，识别性能仅降低 0.79%。这可以解释为姿势信息已经通过对应的编码器表示，因此在表情编码器中保留的信息很少。他们使用混淆损失（confusion loss）来进一步减少他们之间的相关性。</li><li>当不使用 $L_{cos}$ 时，准确性降低了1.67%。这表明在特征空间中使身份和表情正交有助于他们的分离。</li><li>$L_{id}$ 进一步确保生成的图像具有相同的身份，因为仅使用 Lrecon 进行重建是不够可靠的。</li><li>判别器对于合成相应的图像是必需的。即使可以通过 $L_{recon}$ 和 $L_{id}$ 实现生成图像的重构，但它们之间的差异仅在像素级别反映。判别器 D 有助于在标签指导下比较生成的中性和表情面部，从而有利于身份和表情的分离。</li></ol><h1 id="总结" tabindex="-1"><a class="header-anchor" href="#总结" aria-hidden="true">#</a> 总结</h1><ol><li>面部表情识别的性能受到表情特征、身份和姿态特征的相互纠缠的限制。因此本文提出了一种身份和姿解耦耦的面部表情识别框架，名为 IPD-FER，旨在学习更具区分性的表情特征，从而提高识别性能。</li><li>IPD-FER 将面部表示编码为身份、姿态和表情特征的组合。考虑到实际环境数据库不为每个样本提供中性参考，文章使用预训练身份识别模型来表示中性或身份信息。</li><li>通过比较同一个体合成的中性和表情面部，表情特征进一步从身份和姿态中解耦。对五个数据库的评估和可视化结果表明，本文的解耦策略是有效的。</li></ol><h1 id="问题" tabindex="-1"><a class="header-anchor" href="#问题" aria-hidden="true">#</a> 问题</h1><p><strong>如何判断是否收敛</strong> GAN 模型是如何判断是否收敛的，是一个比较复杂的问题，目前没有一个统一的标准或方法。不过，一般来说，有以下几个方面可以参考：</p><ul><li>GAN 模型的理论损失值。如果 GAN 模型使用的是交叉熵损失函数，那么当模型达到纳什平衡状态时，判别器和生成器的损失值都应该接近于 log (0.5)≈0.693¹。但是，这个值并不能保证生成的样本质量和多样性，也不能反映真实数据和合成数据分布之间的差异²。</li><li>GAN 模型的评价指标。为了更好地衡量 GAN 模型的生成效果，一些评价指标被提出，例如 Inception Score (IS)，Fréchet Inception Distance (FID)，Precision and Recall (PR)，等等³。这些指标可以从不同的角度反映生成样本的质量和多样性，但也有一些局限性和偏差。</li><li>GAN 模型的可视化结果。最直观的方法是观察生成样本的可视化结果，看是否清晰，逼真，有没有明显的缺陷或者异常。当然，这种方法也比较主观和难以量化。</li></ul><p>综上所述，GAN 模型是否收敛，需要综合考虑多个方面的信息，而且可能因为不同的任务和数据集而有所差异。希望这些信息对你有所帮助。</p>',43),r=[s];function m(c,o){return a(),t("div",null,r)}const l=e(n,[["render",m],["__file","Disentangling Identity and Pose for Facial Expression Recognition.html.vue"]]);export{l as default};
