import{_ as o,p as t,q as e,R as s,t as _}from"./framework-e03faf0e.js";const a={},c=s("p",null,"对于大多数CNN网络，我们一般是使用L2-loss而不是L1-loss，因为L2-loss的收敛速度要比L1-loss要快得多。",-1),l=s("p",null,"对于边框预测回归问题，通常也可以选择平方损失函数（L2损失），但L2范数的缺点是当存在离群点（outliers)的时候，这些点会占loss的主要组成部分。",-1),n=s("p",null,[_("smooth L1的解决办法是在0点附近使用平方函数使得它更加平滑。 $$ Smooth\\quad L_1=\\begin{matrix}0.5x^2,\\quad|x|<1\\ |x|-0.5,\\quad x<-1or x>1\\end{matrix} $$ "),s("img",{src:"https://raw.githubusercontent.com/0Eumenides/upic/main/2023/04/25/v2-94d674b0173e8b2a81a4aa2c42dc4575_1440w.jpeg",alt:"img"})],-1),r=[c,l,n];function i(d,m){return t(),e("div",null,r)}const h=o(a,[["render",i],["__file","smooth_L1_loss.html.vue"]]);export{h as default};
