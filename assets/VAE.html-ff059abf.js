import{_ as e,p as i,q as a,a1 as s}from"./framework-e03faf0e.js";const t={},n=s(`<div class="language-embed line-numbers-mode" data-ext="embed"><pre class="language-embed"><code>title: &#39;[论文简析]VAE: Auto-encoding Variational Bayes[1312.6114]_哔哩哔哩_bilibili&#39;
image: &#39;https://i2.hdslb.com/bfs/archive/32a4cea5038b1139545f6343f31252e7190ee6ae.jpg@100w_100h_1c.png&#39;
description: &#39;论文题目: Auto-encoding Variational Bayes论文地址: https://arxiv.org/abs/1312.6114大概是要讲的最早的一篇论文，断断续续分了三天录制节奏有些微变化。* 本视频旨在隔离期间维持up思维清晰能说人话，受能力限制经常出现中英混杂，散装英语等现象，请见谅。涉及论文理解报道出了偏差，欢迎各位怒斥。, 视频播放量 19600、弹幕量 52、点赞数 555、投硬币枚数 442、收藏人数 905、转发人数 111, 视频作者 秋刀鱼的炼丹工坊, 作者简介 经中此篇如此高深，我确实不懂。，相关视频：【变分自编码器VAE】可视化讲明白，变分自编码 V…&#39;
url: &#39;https://www.bilibili.com/video/BV1q64y1y7J2/?spm_id_from=333.337.search-card.all.click&amp;vd_source=8c226021e84e474b5fd2a3b0a3e33462&#39;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h1 id="训练" tabindex="-1"><a class="header-anchor" href="#训练" aria-hidden="true">#</a> 训练</h1><p><img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/Xwgsem.jpg" alt="Xwgsem"> 因为中间的 latent value $Z$ 是一种分布（如高斯分布），因此就可以采样出多组，经过 Decoder 生成多种与原图相似却又不同的图像 损失函数： <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/Htw6gp.jpg" alt="Htw6gp"> 使用重参数技巧引入随机变量 $\\epsilon \\sim \\mathcal{N}(0,1)$ 到 latent value $Z$，这就能避免靠采样计算梯度导致的大方差带来的训练不稳定问题，使得可以反向传播用很好的计算出梯度 同时利用了 KL 散度的性质和 MSE 损失函数，来计算损失大小 VAE 的损失函数中 KL 散度是用来衡量编码后的隐变量分布 q (z|x)与先验分布 p (z)之间的差异的。VAE 的目标是让 q (z|x)尽可能地接近 p (z)，而 p (z)通常假设为<strong>标准高斯分布</strong>。这样做的好处是可以从标准高斯分布中采样 z，然后通过解码器生成 x，实现从隐空间到数据空间的映射。另外，如果 q (z|x)趋于标准高斯分布，那么 KL 散度项就会趋于 0，这样可以避免 KL 散度消失的问题。 <img src="https://raw.githubusercontent.com/0Eumenides/upic/main/2023/05/14/UnmcY0.jpg" alt="UnmcY0"></p><ol><li><strong>输入</strong>：首先，我们的输入是一个 32 x 32 大小的图像，有 3 个通道。所以输入的 shape 是 <code>(batch_size, 3, 32, 32)</code>。</li><li><strong>编码器</strong>：输入图像被送入编码器，经过一系列的卷积层和激活函数，每一层都可能改变数据的 shape。例如，如果我们使用 stride 为 2 的卷积，那么每经过一层，图像的宽度和高度都会减半，同时通道数可能增加。假设我们有两个这样的卷积层，那么在编码器的最后，数据的 shape 可能是 <code>(batch_size, 64, 8, 8)</code>。</li><li><strong>潜在向量</strong>：编码器的输出被映射到一个潜在向量。这通常是通过一个全连接层来实现的，将上一步的输出展平，然后通过全连接层得到潜在向量。假设我们的潜在向量的长度是 128，那么这一步后，数据的 shape 会是 <code>(batch_size, 128)</code>。</li><li><strong>解码器</strong>：潜在向量被送入解码器。解码器的任务是将这个向量解码成一个图像。通常，这是通过一系列的反卷积（也叫转置卷积）层和激活函数来实现的。每一层都可能改变数据的 shape。例如，如果我们使用 stride 为 2 的反卷积，那么每经过一层，图像的宽度和高度都会加倍，同时通道数可能减少。假设我们有两个这样的反卷积层，那么在解码器的最后，数据的 shape 可能是 <code>(batch_size, 3, 32, 32)</code>，这和输入的 shape 一致。</li></ol><h1 id="推理" tabindex="-1"><a class="header-anchor" href="#推理" aria-hidden="true">#</a> 推理</h1><p>在变分自动编码器（VAE）中，编码器的目标是学习数据的潜在分布，并尝试将其映射到一个标准的高斯分布（即均值为0，方差为1的分布）。在训练期间，给定输入图像 x，编码器会输出一组参数（通常是均值和方差），这组参数描述了一个高斯分布，我们从这个分布中抽样得到潜在变量 z。 然而，在测试（或称为推理）期间，我们的目标是生成新的样本。这时，我们不再需要输入图像x，而是直接从标准高斯分布N(0,1)中抽样得到潜在变量z，然后将z送入解码器，生成新的图像。 这就是为什么在测试期间，潜在变量z的分布是标准高斯分布N(0,1)，而不是依赖于输入x的。这是因为VAE的目标是学习生成数据的潜在分布，而不仅仅是复制输入的样本。</p>`,6),c=[n];function r(d,o){return i(),a("div",null,c)}const h=e(t,[["render",r],["__file","VAE.html.vue"]]);export{h as default};
